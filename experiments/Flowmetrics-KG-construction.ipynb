{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc3ac46-9cdd-457a-b496-f3127edc2250",
   "metadata": {},
   "source": [
    "# Flowmetrics – Impact-Augmented Knowledge Graph Construction\n",
    "\n",
    "This notebook constructs the Impact-Augmented Knowledge Graph, the central data structure underpinning the Flowmetrics framework. It documents the end-to-end pipeline for generating a structured dataset of societal research impact trajectories by integrating heterogeneous data sources into an RDF graph suitable for AI-driven impact modelling.\n",
    "\n",
    "### Objective\n",
    "\n",
    "To automate the extraction, semantic alignment, and structuring of research topic pairs and their associated impact signals — enabling scalable modelling of how research impact unfolds across time, platforms, and audiences.\n",
    "\n",
    "### Structure\n",
    "\n",
    "#### 1. Data Collection Pipeline  \n",
    "Harvests impact evidence through API integration with three key platforms:  \n",
    "- **arXiv** – source of metadata for topic extraction and co-occurrence modelling  \n",
    "- **Altmetric** – provider of online attention signals (e.g., news, social media, blogs)  \n",
    "- **CrossRef** – supplier of citation-based and policy-linked influence metrics\n",
    "\n",
    "The pipeline operates on a curated corpus of 12,350 computer science preprints (2000–2024), spanning major subfields such as machine learning, natural language processing, computer vision, and artificial intelligence. Papers were selected for topical diversity, metadata completeness, and coverage across at least one impact platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11db9d-524c-4f8f-a224-fa35af4d68fb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Data collection pipeline: automating data extraction](#section-1)\n",
    "  - [1.1 API Integration](#subsection-11)\n",
    "     - [1.1.1 Altmetric Data](#subsection-111)\n",
    "     - [1.1.2 CrossRef Data](#subsection-112)\n",
    "     - [1.1.3 arXiv Data](#subsection-113)\n",
    "  - [1.2 Impact Trajectory Matching](#subsection-12)\n",
    "     - [1.2.1 Nodes of topics](#subsection-121)\n",
    "     - [1.2.2 Edges of impact](#subsection-122)\n",
    "     - [1.2.3 Impact-augmented knowledge graph](#subsection-123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40cd920f-bd51-4f1c-b084-b652a5f76b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from cso_classifier import CSOClassifier\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f5401fc-0fb7-4b33-aeb5-11450e20571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path(\".\").resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab467c9e-bd29-4c38-b3bf-d1355b57cffc",
   "metadata": {},
   "source": [
    "## 1.1 API Integration  \n",
    "### 1.1.1 Altmetric Data\n",
    "\n",
    "Altmetric data captures the online attention and engagement surrounding scholarly publications, providing insight into the broader impact of research beyond traditional citation metrics. It reflects how a paper is being discussed, shared, and interacted with across a range of platforms including news outlets, blogs, Twitter, Facebook, Reddit, Wikipedia, and policy documents.\n",
    "\n",
    "This data helps construct a multidimensional view of research visibility and societal relevance — spanning public discourse, academic engagement, and policy uptake. As such, Altmetric indicators are increasingly valuable for researchers, institutions, and funders aiming to understand and quantify how research resonates across different audiences and sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "790c3b86-c2d9-4fa3-b4cb-f69023edf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altmetric_data(doi):\n",
    "    if not doi:\n",
    "        return [None] * 12  # Return None for all counts if DOI is invalid\n",
    "\n",
    "    url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Use dictionary's get method to return None for missing keys instead of defaulting to 0\n",
    "            return [\n",
    "                data.get(\"score\"),\n",
    "                data.get(\"cited_by_posts_count\"),\n",
    "                data.get(\"cited_by_tweeters_count\"),\n",
    "                data.get(\"cited_by_news_outlets_count\"),\n",
    "                data.get(\"cited_by_blogs_count\"),\n",
    "                data.get(\"cited_by_rdts_count\"),\n",
    "                data.get(\"cited_by_fbwalls_count\"),\n",
    "                data.get(\"cited_by_patents_count\"),\n",
    "                data.get(\"cited_by_wikipedia_count\"),\n",
    "                data.get(\"cited_by_policy_count\"),\n",
    "                data.get(\"cited_by_mendeley_count\"),\n",
    "                data.get(\"cited_by_videos_count\")\n",
    "            ]\n",
    "        \n",
    "        # Handle different status codes\n",
    "        elif response.status_code == 401:\n",
    "            print(\"Unauthorized: Check your API key or permissions.\")\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded. Please try again later.\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network or request error: {e}\")\n",
    "    \n",
    "    # Return None values if the request fails or DOI is invalid\n",
    "    return [None] * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261a740-a68b-4f5a-ad7e-7f50038d784a",
   "metadata": {},
   "source": [
    "### 1.1.2 CrossRef Data\n",
    "\n",
    "CrossRef data provides extensive metadata and citation information for scholarly works, including journal articles, books, conference proceedings, datasets, and other research outputs. It plays a critical role in enhancing the discoverability and traceability of academic content through the use of persistent identifiers such as Digital Object Identifiers (DOIs).\n",
    "\n",
    "Within the Flowmetrics framework, CrossRef serves as a key source of citation-based impact signal. These signals help trace the academic and institutional reach of research over time, offering a complementary view to socially-driven metrics and enabling a more comprehensive understanding of scholarly influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a753e3ce-70c8-4c23-91b8-09a132f7a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_count_from_crossref(doi):\n",
    "    if not doi:\n",
    "        return None  # Return None immediately if DOI is invalid\n",
    "\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('message', {}).get('is-referenced-by-count', 0)\n",
    "\n",
    "        # Handle common error status codes\n",
    "        if response.status_code == 404:\n",
    "            pass\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded. Please try again later.\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network or request error: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a980bca-00f4-448b-b8ac-7f4d8ab396c7",
   "metadata": {},
   "source": [
    "### 1.1.3 arXiv Data\n",
    "\n",
    "arXiv is a widely used preprint repository that offers open access to scholarly articles across a broad range of disciplines, including computer science, physics, mathematics, statistics, electrical engineering, quantitative biology, and economics.\n",
    "\n",
    "In the Flowmetrics framework, arXiv serves as the primary source for research metadata, enabling the extraction of topics and co-occurrence patterns. This metadata provides the structural backbone for identifying topic pairs and aligning them with downstream impact signals collected from Altmetric and CrossRef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7e09970-b342-4511-94be-3fb99d35fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = project_dir / \"data\" / \"arxiv-metadata-oai-snapshot.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f26a388b-aba9-4f1e-b22a-26e313f40d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {\n",
    "    'cs.AI': 'Artificial Intelligence',\n",
    "    'cs.CL': 'Computation and Language',\n",
    "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "    'cs.DS': 'Data Structures and Algorithms',\n",
    "    'cs.ET': 'Emerging Technologies',\n",
    "    'cs.HC': 'Human-Computer Interaction',\n",
    "    'cs.IR': 'Information Retrieval',\n",
    "    'cs.NE': 'Neural and Evolutionary Computing',\n",
    "    'cs.LG': 'Machine Learning'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1451024-1617-4ded-b4f3-6f86f39fbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return ' '.join(filtered_sentence).replace(',','')\n",
    "\n",
    "def lemmatizer_word(word):\n",
    "    return wnl.lemmatize(word)\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer_word(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def get_metadata():\n",
    "    with open(DATASET_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32e78932-1acf-4a22-a15f-9f11097a7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'dois': [], 'titles': [], 'abstracts': [], 'years': [], 'categories': [],\n",
    "    'citations_crossref': [], 'citations_wos': [], 'altmetric_score': [], \n",
    "    'all_mentions_counts': [], 'twitter_counts': [], 'news_counts': [],\n",
    "    'blogs_counts': [], 'reddit_counts': [], 'facebook_counts': [],\n",
    "    'patents_counts': [], 'wiki_counts': [], 'policy_counts': [],\n",
    "    'mendeley_counts': [], 'video_counts': [], 'views': [], 'downloads': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d15baf-7e48-4bce-9a17-a3a1ce5d6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()\n",
    "\n",
    "# Process metadata and populate the data dictionary\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    ref = paper_dict.get('journal-ref')\n",
    "    doi = paper_dict.get('doi')\n",
    "    try:\n",
    "        year = int(ref[-4:])\n",
    "        if 2000 < year <= 2024:\n",
    "            category = paper_dict.get('categories').split(\" \")[0]\n",
    "            data['categories'].append(category_map.get(category, 'Unknown'))  # Handle missing category mapping\n",
    "            data['years'].append(year)\n",
    "            data['titles'].append(lemmatizer(remove_stop_words(paper_dict.get('title', ''))))\n",
    "            data['abstracts'].append(lemmatizer(remove_stop_words(paper_dict.get('abstract', ''))))\n",
    "            data['dois'].append(doi)\n",
    "            data['citations_crossref'].append(get_citation_count_from_crossref(doi))\n",
    "            \n",
    "            altmetric_data = get_altmetric_data(doi)\n",
    "            data['altmetric_score'].append(altmetric_data[0])\n",
    "            data['all_mentions_counts'].append(altmetric_data[1])\n",
    "            data['twitter_counts'].append(altmetric_data[2])\n",
    "            data['news_counts'].append(altmetric_data[3])\n",
    "            data['blogs_counts'].append(altmetric_data[4])\n",
    "            data['reddit_counts'].append(altmetric_data[5])\n",
    "            data['facebook_counts'].append(altmetric_data[6])\n",
    "            data['patents_counts'].append(altmetric_data[7])\n",
    "            data['wiki_counts'].append(altmetric_data[8])\n",
    "            data['policy_counts'].append(altmetric_data[9])\n",
    "            data['mendeley_counts'].append(altmetric_data[10])\n",
    "            data['video_counts'].append(altmetric_data[11])\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        pass\n",
    "\n",
    "# Return the lengths of the lists to check if they match\n",
    "len(data['dois']), len(data['titles']), len(data['abstracts']), len(data['years']), len(data['categories']), len(data['citations_crossref']), len(data['altmetric_score']), len(data['views']), len(data['downloads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55375943-b3a8-4041-826a-33eefebdcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'DOI': dois,\n",
    "    'Title': titles,\n",
    "    'Abstract': abstracts,\n",
    "    'Year': years,\n",
    "    'Category': categories,\n",
    "    'Citation_crossref': citations_crossref,\n",
    "    'Altmetric_score': altmetric_score,\n",
    "    'All_mentions': all_mentions_counts,\n",
    "    'Twitter': twitter_counts,\n",
    "    'News': news_counts,\n",
    "    'Blogs': blogs_counts,\n",
    "    'Reddit': reddit_counts,\n",
    "    'Facebook': facebook_counts,\n",
    "    'Patents': patents_counts,\n",
    "    'Policy': policy_counts,\n",
    "    'Mendeley': mendeley_counts,\n",
    "    'Wikipedia': wiki_counts,\n",
    "    'Videos': video_counts,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d514f7-8449-4292-a9ba-14eb453f69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pre-processed files\n",
    "#DATASET_PATH = project_dir / \"data\" / \"processed\" / \"flow_dataset.csv\"\n",
    "#df.to_csv(DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a8fa9-34fe-4298-baf1-d1a381a7fc33",
   "metadata": {},
   "source": [
    "## 1.2 Impact Trajectory Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060e755-f252-414d-baeb-6e69c500ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2f365-4dc4-40a9-8e4a-76deca699b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3deea-79f0-4d91-929d-9438bea7cd3c",
   "metadata": {},
   "source": [
    "### 1.2.1 Nodes of Topics\n",
    "\n",
    "The nodes of the knowledge graph represent high-quality research topics extracted from arXiv metadata. To identify these topics, we used the **CSO Classifier** — an unsupervised tool that assigns concepts from the **Computer Science Ontology (CSO)** based on paper titles, abstracts, and keywords.\n",
    "\n",
    "The CSO Classifier integrates two components:  \n",
    "- A **syntactic module**, which detects explicitly mentioned concepts  \n",
    "- A **semantic module**, which leverages part-of-speech tagging and word embeddings to infer related concepts\n",
    "\n",
    "Outputs from both modules are merged to generate a candidate topic list for each paper. To consolidate fine-grained variations, we applied a **frequency-based clustering strategy**, assigning each paper to its most frequently associated concept across the corpus. This allowed for the grouping of semantically similar papers under unified topic identifiers without manual filtering.\n",
    "\n",
    "The resulting pipeline generated **156 coherent, domain-relevant topic nodes**, covering diverse areas of computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd66fd-7190-4d83-88de-c25478a4539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install CSOClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ea1a8-2013-4bae-a2f7-ade5d4c9f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CSOClassifier(modules = \"both\", enhancement = \"all\", explanation = False)\n",
    "results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e0d36-7812-40a4-b90e-863dd33a15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_dict(row):\n",
    "    paper = {\n",
    "        \"title\": row[\"Title\"],\n",
    "        \"abstract\": row[\"Abstract\"]\n",
    "    }\n",
    "    return cc.run(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82873fd-5a19-4e74-b8b5-5b16bdf62d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to create the 'cso_topics' column\n",
    "df[\"CSO_classifier\"] = df.apply(create_paper_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c138c-e7f8-4135-a0e8-439d3f93b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.head(10).iterrows():\n",
    "    print(f\"Category: {row['Category']}\")\n",
    "    print(f\"Title: {row['Title']}\")\n",
    "    print(f\"Abstract: {row['Abstract']}\")\n",
    "    print(f\"CSO Topics: \\n{json.dumps(row['CSO_classifier'], indent=4)}\\n\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d06ec6-b228-460f-99ab-87036aeb07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"union\" list from each dictionary\n",
    "df[\"Research_concepts\"] = df[\"CSO_classifier\"].apply(lambda x: x.get(\"union\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bfa89-0103-4916-a71a-d3bb776b0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Super_topics\"] = df[\"CSO_classifier\"].apply(lambda x: x.get(\"enhanced\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217dae7f-415b-42d9-a7e2-703aa6948e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Research_concepts\"] = df[\"Research_concepts\"].apply(ast.literal_eval)\n",
    "all_concepts = df[\"Research_concepts\"].apply(lambda x: x).explode().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297974f-1106-4712-a4d9-2348a0e03b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the frequency of each concept across all papers\n",
    "concept_counts = pd.Series(all_concepts).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c1821-ad41-489b-8fd2-f79436f1a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Assign the most frequent concept as the cluster for each paper\n",
    "df[\"Name\"] = df[\"Research_concepts\"].apply(\n",
    "    lambda concepts: max(concepts, key=lambda concept: concept_counts.get(concept, 0))\n",
    ")\n",
    "# Step 3: Generate unique Topic IDs starting from 0\n",
    "df[\"Topic\"] = df[\"Name\"].map({name: idx for idx, name in enumerate(df_dataset[\"Name\"].unique())})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf1dde-1483-40a2-afbc-da214b76b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure feedback columns exist in df, even if filled with 0.0\n",
    "for col in [\"Peers\", \"Expert\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76483d5-9fef-4fea-952f-cf638360a9d9",
   "metadata": {},
   "source": [
    "### 1.2.2 Edges of Impact\n",
    "\n",
    "Edges in the Impact-Augmented Knowledge Graph represent impact-bearing relationships between research topics. An edge is established when two topics co-occur in at least one paper and share a common CSO concept. These relationships are encoded as RDF triples of the form *(TopicPair, hasStageImpact, BlankNode)*.\n",
    "\n",
    "Each `hasStageImpact` predicate is instantiated with a stage-specific property (e.g., `flow:hasReachImpact`, `flow:hasInfluenceImpact`). The associated `BlankNode` stores metadata about the impact source, including:  \n",
    "- `flow:platform`: the platform generating the signal (e.g., Twitter, Facebook)  \n",
    "- `flow:score`: the aggregated and normalised impact score for that platform and stage\n",
    "\n",
    "#### Aggregation Strategy\n",
    "\n",
    "To produce these edge-level scores, we applied a two-step aggregation process:  \n",
    "1. **Normalisation:** Raw scores were independently normalised per platform and dimension to account for scale differences (e.g., Altmetric vs. CrossRef).  \n",
    "2. **Summation:** For each topic pair and stage, we summed the normalised scores across all platforms associated with that dimension.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider the topic pair `Semantics` and `Language Model`, which co-occur and share the CSO concept `language model`. Their Reach impact is encoded as the following RDF triples:  \n",
    "*(flow:pair_15_131, flow:hasReachImpact, :reach1)*,  \n",
    "*(:reach1, flow:platform, \"Twitter\")*,  \n",
    "*(:reach1, flow:score, 0.1321)*,  \n",
    "*(:reach2, flow:platform, \"Facebook\")*,  \n",
    "*(_:reach2, flow:score, 0.0972)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db944d5b-ae67-4c5f-8a32-edb922d4659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(series, range_min, range_max):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    return ((series - min_val) / (max_val - min_val) * (range_max - range_min) + range_min).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217fea0-00c0-48e6-b35b-de278bc7267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_concepts(concepts):\n",
    "    concepts_count = pd.Series(concepts).value_counts()\n",
    "    top_10_concepts = concepts_count.nlargest(10).index.tolist()\n",
    "    top_10_counts = concepts_count.nlargest(10).values.tolist()\n",
    "    top_10_concepts_with_counts = list(zip(top_10_concepts, top_10_counts))\n",
    "    return top_10_concepts_with_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedd6d1-5507-4330-9717-72ab046cb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowmetrics labeling\n",
    "reach_columns = [\"Twitter\", \"Facebook\", \"Wikipedia\"]\n",
    "engagement_columns = [\"Blogs\", \"Reddit\", \"Videos\", \"Mendeley\"]\n",
    "feedback_columns = [\"Peers\", \"Expert\"]\n",
    "influence_columns = [\"News\", \"Citation_crossref\"]\n",
    "outcomes_columns = [\"Policy\", \"Patents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e998f90-89f4-41ac-81c7-786098c36fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_scores = df_dataset.groupby([\"Topic\", \"Name\"]).agg({\n",
    "    \"Twitter\": \"sum\",\n",
    "    \"Facebook\": \"sum\",\n",
    "    \"Reddit\": \"sum\",\n",
    "    \"Wikipedia\": \"sum\",\n",
    "    \"Blogs\": \"sum\",\n",
    "    \"Videos\": \"sum\",\n",
    "    \"Mendeley\": \"sum\",\n",
    "    \"Peers\": \"sum\",\n",
    "    \"Expert\": \"sum\",\n",
    "    \"News\": \"sum\",\n",
    "    \"Citation_crossref\": \"sum\",\n",
    "    \"Policy\": \"sum\",\n",
    "    \"Patents\": \"sum\",\n",
    "    \"Research_concepts\": lambda x: [concept for sublist in x for concept in sublist]\n",
    "}).reset_index()\n",
    "\n",
    "agg_scores['Count'] = df_dataset.groupby([\"Topic\", \"Name\"]).size().values\n",
    "agg_scores['Count_norm'] = normalize(df_dataset.groupby([\"Topic\", \"Name\"]).size().values, 40, 100)\n",
    "agg_scores[\"Reach\"] = agg_scores[reach_columns].apply(lambda row: list(zip(reach_columns, row)), axis=1)\n",
    "agg_scores[\"Engagement\"] = agg_scores[engagement_columns].apply(lambda row: list(zip(engagement_columns, row)), axis=1)\n",
    "agg_scores[\"Feedback\"] = agg_scores[feedback_columns].apply(lambda row: list(zip(feedback_columns, row)), axis=1)\n",
    "agg_scores[\"Influence\"] = agg_scores[influence_columns].apply(lambda row: list(zip(influence_columns, row)), axis=1)\n",
    "agg_scores[\"Outcome\"] = agg_scores[outcomes_columns].apply(lambda row: list(zip(outcomes_columns, row)), axis=1)\n",
    "agg_scores[\"Representation\"] = agg_scores[\"Research_concepts\"].apply(get_top_10_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c1f4e-b78d-4dfd-8f6a-e65a738919f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove topics where the 'Representation' list has fewer than 10 concepts\n",
    "agg_scores = agg_scores[agg_scores[\"Representation\"].apply(lambda x: len(x) == 10)].reset_index(drop=True)\n",
    "agg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a22f4-31f9-498c-ba6e-b0db01a1cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "\n",
    "for idx, row in agg_scores.iterrows():\n",
    "    topics[idx] = {\n",
    "        'topic': row['Name'],\n",
    "        'cluster_size': row['Count_norm'],\n",
    "        'reach': row['Reach'],\n",
    "        'engagement': row['Engagement'],\n",
    "        'feedback': row['Feedback'],\n",
    "        'influence': row['Influence'],\n",
    "        'outcome': row['Outcome'],\n",
    "        'words': row['Representation']  # List of tuples (concept, count)\n",
    "    }\n",
    "#import pprint\n",
    "#pprint.pprint(dict(list(topics.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f7fe5-48b4-426a-b64c-d0e2419a1d4a",
   "metadata": {},
   "source": [
    "### 1.2.3 Impact-augmented knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a5fc1-74a5-4260-88be-c13aa811ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_edge_weights(G, weight_attr=\"width\", new_range=(0, 1)):\n",
    "    a, b = new_range\n",
    "    weights = [G[u][v][weight_attr] for u, v in G.edges]\n",
    "    min_weight = min(weights)\n",
    "    max_weight = max(weights)\n",
    "    for u, v in G.edges:\n",
    "        original_weight = G[u][v][weight_attr]\n",
    "        normalized_weight = a + ((original_weight - min_weight) * (b - a)) / (max_weight - min_weight)\n",
    "        G[u][v][weight_attr] = round(normalized_weight, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbcf24-5216-490d-9af0-d87e12cc69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for topic, value in topics.items():\n",
    "    topic_node = f\"Topic {topic}\"\n",
    "    G.add_node(topic_node, size=(value['cluster_size']), label=topic_node, type='topic')\n",
    "    \n",
    "    for word, _ in value.get(\"words\", [])[:10]: #[:100]\n",
    "        G.add_node(word, size=10, label=word, type='leaf')\n",
    "        G.add_edge(topic_node, word, weight=1)\n",
    "\n",
    "overlap_leafs = []\n",
    "for topic1, value1 in topics.items():\n",
    "    for topic2, value2 in topics.items():\n",
    "        if topic1 != topic2:\n",
    "            words1 = set([word[0] for word in value1.get(\"words\", [])])\n",
    "            words2 = set([word[0] for word in value2.get(\"words\", [])])\n",
    "            common_words = words1.intersection(words2)\n",
    "            if common_words:\n",
    "                overlap_leafs.append(next(iter(common_words)))\n",
    "                G.add_edge(f\"Topic {topic1}\", f\"Topic {topic2}\", color=\"#4caf50\", weight=len(common_words) + \n",
    "                           float(sum(value for _, value in value1.get(\"reach\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"reach\", []))) + \n",
    "                           float(sum(value for _, value in value1.get(\"engagement\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"engagement\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"feedback\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"feedback\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"influence\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"influence\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"outcome\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"outcome\", []))))\n",
    "\n",
    "# Normalize weights\n",
    "normalize_edge_weights(G, weight_attr=\"weight\", new_range=(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74bdb2-71ce-470b-9893-e1f7f6ddd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height=\"800px\", width=\"100%\", notebook=True, bgcolor=\"white\", font_color=\"black\") ##f0f0f0\n",
    "net.from_nx(G)\n",
    "\n",
    "for node in net.nodes:\n",
    "    if node['type'] == \"topic\":\n",
    "        node['borderWidth'] = 4\n",
    "        node['shadow'] = True\n",
    "        node['color'] = \"#9e9e9e\"\n",
    "    else:\n",
    "        if node['id'] in overlap_leafs:\n",
    "            node['borderWidth'] = 3\n",
    "            node['shadow'] = True\n",
    "            node['color'] = \"#ff9800\"\n",
    "        else:\n",
    "            node['borderWidth'] = 3\n",
    "            node['shadow'] = True\n",
    "            node['color'] = \"#616161\"\n",
    "\n",
    "for edge in net.edges:\n",
    "    weight = edge.get('width', 1)\n",
    "    edge['label'] = str(weight)\n",
    "    edge['font'] = {'size': 8}\n",
    "    \n",
    "net.force_atlas_2based()\n",
    "net.show(\"G_topic_knowledge_graph.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc3ac46-9cdd-457a-b496-f3127edc2250",
   "metadata": {},
   "source": [
    "# Flowmetrics – LLM Classification\n",
    "\n",
    "This notebook defines and implements the core classification task of the Flowmetrics framework. It formalises research impact modelling as a two-part problem: (i) predicting impact stages for research topic pairs, and (ii) generating narrative summaries that explain these impact trajectories based on structured evidence.\n",
    "\n",
    "### Objective\n",
    "\n",
    "To use large language models (LLMs) in a zero-shot setting to predict the stage-wise impact of research topic pairs and generate structured explanations of how research unfolds across societal dimensions.\n",
    "\n",
    "### Structure\n",
    "\n",
    "#### 1. Impact Stage Prediction  \n",
    "Given a topic pair \\((t_A, t_B)\\), the model assigns one or more impact stages from the Flowmetrics trajectory:\n",
    "- **Reach** – Twitter, Facebook, Wikipedia  \n",
    "- **Engagement** – Blogs, Reddit, YouTube, Mendeley  \n",
    "- **Feedback** – *Defined in the framework, but not associated with publicly available signals*  \n",
    "- **Influence** – CrossRef citations, News mentions  \n",
    "- **Outcome** – Policy mentions, Patent citations\n",
    "\n",
    "Although the **Feedback** stage is part of the framework — representing expert critique and scholarly evaluation — it is not currently operationalised due to the lack of observable public signals. Incorporating this dimension remains a future direction, especially with the emergence of open peer review datasets.\n",
    "\n",
    "#### 2. Narrative Impact Generation  \n",
    "Beyond stage classification, the model generates a concise narrative for each topic pair based on the predicted stages and structured metric evidence. This defines the task as a form of **structured data-to-text generation**, where the input graph encodes impact-stage associations and the output provides a coherent account of societal impact.\n",
    "\n",
    "### Prompting Strategy\n",
    "\n",
    "We employ a standardised zero-shot prompt template to ensure consistent model behaviour. The prompt encourages inclusive reasoning across weak or cumulative signals. When no platform data is detected for a topic pair, the model assigns **Reach** by default — signalling minimal public accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11db9d-524c-4f8f-a224-fa35af4d68fb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Classification pipeline: automating Flowmetrics labeling](#section-1)\n",
    "    - [1.1 Training](#subsection-11)\n",
    "        - [1.1.1 Load model](#subsection-111)\n",
    "        - [1.1.2 Prompt templates for zero-shot learning](#subsection-112)\n",
    "    - [1.2 Prediction](#subsection-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40cd920f-bd51-4f1c-b084-b652a5f76b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "from itertools import combinations\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from rdflib import Graph, Namespace, URIRef, RDFS, XSD, RDF\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from config import models, impact_stages, stage_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0f5401fc-0fb7-4b33-aeb5-11450e20571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path(\".\").resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0332d-68d4-4989-84df-f7e2f506034b",
   "metadata": {},
   "source": [
    "## 1. Classification Pipeline: Automating Flowmetrics Labeling\n",
    "\n",
    "This section defines the zero-shot classification pipeline used to assign Flowmetrics impact stages to research topic pairs. The pipeline leverages large language models (LLMs) to predict one or more stages based on structured metric inputs derived from Altmetric and CrossRef.\n",
    "\n",
    "Each input instance represents a topic pair, accompanied by evidence of platform-level engagement. The LLM is prompted to reason over these signals and return the most appropriate stages from the predefined typology: **Reach**, **Engagement**, **Feedback**, **Influence**, and **Outcome**.\n",
    "\n",
    "The classification is performed in a zero-shot setting using a standardised prompt template, which:\n",
    "- Encourages cumulative and inclusive reasoning across weak or sparse evidence\n",
    "- Defaults to **Reach** when no platform signals are present\n",
    "- Supports multi-label output to reflect the multidimensional nature of societal impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f7e09970-b342-4511-94be-3fb99d35fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FILE = project_dir / \"data\" / \"impact_augmented_kg.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ec485a39-8a2e-4a91-8325-759635f901a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nb58774477a044f9aa5ab958f400800c8 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(GRAPH_FILE, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e3455f8-d1a1-446d-82d9-cf5b4842a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define namespaces\n",
    "FLOW = rdflib.Namespace(\"http://example.org/flowmetrics#\")\n",
    "RDFS = rdflib.Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = rdflib.RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "294ae883-3cf8-4d9f-9508-c9fea1e2cda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1_id</th>\n",
       "      <th>topic_1_name</th>\n",
       "      <th>topic_2_id</th>\n",
       "      <th>topic_2_name</th>\n",
       "      <th>shared_concepts</th>\n",
       "      <th>reach</th>\n",
       "      <th>engagement</th>\n",
       "      <th>feedback</th>\n",
       "      <th>influence</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://example.org/flowmetrics#topic_0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>http://example.org/flowmetrics#topic_1</td>\n",
       "      <td>combinatorial problems</td>\n",
       "      <td>[combinatorial optimization, combinatorial pro...</td>\n",
       "      <td>[(Wikipedia, 0.2784810126582279), (Facebook, 0...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.14545454545454545), ...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.14096521945183116), (Ne...</td>\n",
       "      <td>[(Patents, 0.07159904534606205), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://example.org/flowmetrics#topic_0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>http://example.org/flowmetrics#topic_10</td>\n",
       "      <td>adaptive algorithms</td>\n",
       "      <td>[combinatorial optimization, combinatorial pro...</td>\n",
       "      <td>[(Wikipedia, 0.3417721518987342), (Twitter, 0....</td>\n",
       "      <td>[(Videos, 0.16666666666666666), (Mendeley, 0.0...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.1498249124562281), (New...</td>\n",
       "      <td>[(Patents, 0.07995226730310262), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://example.org/flowmetrics#topic_0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>http://example.org/flowmetrics#topic_113</td>\n",
       "      <td>optimization problems</td>\n",
       "      <td>[combinatorial optimization, combinatorial pro...</td>\n",
       "      <td>[(Wikipedia, 0.2468354430379747), (Facebook, 0...</td>\n",
       "      <td>[(Mendeley, 0.0), (Reddit, 0.09090909090909091...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.12760327532187146), (Ne...</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.06205250596658711)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://example.org/flowmetrics#topic_0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>http://example.org/flowmetrics#topic_138</td>\n",
       "      <td>convolutional neural networks</td>\n",
       "      <td>[computational efficiency]</td>\n",
       "      <td>[(Facebook, 0.1388888888888889), (Wikipedia, 0...</td>\n",
       "      <td>[(Reddit, 0.1090909090909091), (Videos, 0.1666...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 0.1301703483...</td>\n",
       "      <td>[(Patents, 0.06563245823389022), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://example.org/flowmetrics#topic_0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>http://example.org/flowmetrics#topic_143</td>\n",
       "      <td>evolutionary algorithms</td>\n",
       "      <td>[evolutionary algorithms, multi-objective opti...</td>\n",
       "      <td>[(Twitter, 0.10295224554654178), (Facebook, 0....</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.09090909090909091), ...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 0.1256549327...</td>\n",
       "      <td>[(Patents, 0.06205250596658711), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>http://example.org/flowmetrics#topic_31</td>\n",
       "      <td>computational efficiency</td>\n",
       "      <td>http://example.org/flowmetrics#topic_9</td>\n",
       "      <td>detection algorithm</td>\n",
       "      <td>[correlation analysis, encoder-decoder]</td>\n",
       "      <td>[(Facebook, 0.14583333333333331), (Wikipedia, ...</td>\n",
       "      <td>[(Reddit, 0.07272727272727272), (Videos, 0.0),...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.11778257549827545), (Ne...</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.08591885441527446)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>http://example.org/flowmetrics#topic_66</td>\n",
       "      <td>reference image</td>\n",
       "      <td>http://example.org/flowmetrics#topic_9</td>\n",
       "      <td>detection algorithm</td>\n",
       "      <td>[reference image]</td>\n",
       "      <td>[(Facebook, 0.08333333333333333), (Wikipedia, ...</td>\n",
       "      <td>[(Videos, 0.0), (Blogs, 0.0), (Reddit, 0.07272...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.10234064400621364), (Ne...</td>\n",
       "      <td>[(Patents, 0.06921241050119331), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>http://example.org/flowmetrics#topic_67</td>\n",
       "      <td>classification models</td>\n",
       "      <td>http://example.org/flowmetrics#topic_9</td>\n",
       "      <td>detection algorithm</td>\n",
       "      <td>[color images, reference image]</td>\n",
       "      <td>[(Facebook, 0.09722222222222221), (Twitter, 0....</td>\n",
       "      <td>[(Reddit, 0.10909090909090909), (Mendeley, 0.0...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.12229799110081356), (Ne...</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.07875894988066826)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>http://example.org/flowmetrics#topic_74</td>\n",
       "      <td>computer vision</td>\n",
       "      <td>http://example.org/flowmetrics#topic_9</td>\n",
       "      <td>detection algorithm</td>\n",
       "      <td>[color images, computer vision, reference image]</td>\n",
       "      <td>[(Twitter, 0.0884837333779376), (Wikipedia, 0....</td>\n",
       "      <td>[(Reddit, 0.09090909090909091), (Blogs, 0.0), ...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.12074458281772465), (Ne...</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.09069212410501193)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>http://example.org/flowmetrics#topic_78</td>\n",
       "      <td>anomaly detection</td>\n",
       "      <td>http://example.org/flowmetrics#topic_9</td>\n",
       "      <td>detection algorithm</td>\n",
       "      <td>[anomaly detection]</td>\n",
       "      <td>[(Wikipedia, 0.06962025316455696), (Facebook, ...</td>\n",
       "      <td>[(Mendeley, 0.0), (Reddit, 0.07272727272727272...</td>\n",
       "      <td>[(Expert, 0.0), (Peers, 0.0)]</td>\n",
       "      <td>[(Citation_crossref, 0.10288038756220215), (Ne...</td>\n",
       "      <td>[(Patents, 0.06921241050119331), (Policy, 0.0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>771 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  topic_1_id              topic_1_name  \\\n",
       "0     http://example.org/flowmetrics#topic_0              optimization   \n",
       "1     http://example.org/flowmetrics#topic_0              optimization   \n",
       "2     http://example.org/flowmetrics#topic_0              optimization   \n",
       "3     http://example.org/flowmetrics#topic_0              optimization   \n",
       "4     http://example.org/flowmetrics#topic_0              optimization   \n",
       "..                                       ...                       ...   \n",
       "766  http://example.org/flowmetrics#topic_31  computational efficiency   \n",
       "767  http://example.org/flowmetrics#topic_66           reference image   \n",
       "768  http://example.org/flowmetrics#topic_67     classification models   \n",
       "769  http://example.org/flowmetrics#topic_74           computer vision   \n",
       "770  http://example.org/flowmetrics#topic_78         anomaly detection   \n",
       "\n",
       "                                   topic_2_id                   topic_2_name  \\\n",
       "0      http://example.org/flowmetrics#topic_1         combinatorial problems   \n",
       "1     http://example.org/flowmetrics#topic_10            adaptive algorithms   \n",
       "2    http://example.org/flowmetrics#topic_113          optimization problems   \n",
       "3    http://example.org/flowmetrics#topic_138  convolutional neural networks   \n",
       "4    http://example.org/flowmetrics#topic_143        evolutionary algorithms   \n",
       "..                                        ...                            ...   \n",
       "766    http://example.org/flowmetrics#topic_9            detection algorithm   \n",
       "767    http://example.org/flowmetrics#topic_9            detection algorithm   \n",
       "768    http://example.org/flowmetrics#topic_9            detection algorithm   \n",
       "769    http://example.org/flowmetrics#topic_9            detection algorithm   \n",
       "770    http://example.org/flowmetrics#topic_9            detection algorithm   \n",
       "\n",
       "                                       shared_concepts  \\\n",
       "0    [combinatorial optimization, combinatorial pro...   \n",
       "1    [combinatorial optimization, combinatorial pro...   \n",
       "2    [combinatorial optimization, combinatorial pro...   \n",
       "3                           [computational efficiency]   \n",
       "4    [evolutionary algorithms, multi-objective opti...   \n",
       "..                                                 ...   \n",
       "766            [correlation analysis, encoder-decoder]   \n",
       "767                                  [reference image]   \n",
       "768                    [color images, reference image]   \n",
       "769   [color images, computer vision, reference image]   \n",
       "770                                [anomaly detection]   \n",
       "\n",
       "                                                 reach  \\\n",
       "0    [(Wikipedia, 0.2784810126582279), (Facebook, 0...   \n",
       "1    [(Wikipedia, 0.3417721518987342), (Twitter, 0....   \n",
       "2    [(Wikipedia, 0.2468354430379747), (Facebook, 0...   \n",
       "3    [(Facebook, 0.1388888888888889), (Wikipedia, 0...   \n",
       "4    [(Twitter, 0.10295224554654178), (Facebook, 0....   \n",
       "..                                                 ...   \n",
       "766  [(Facebook, 0.14583333333333331), (Wikipedia, ...   \n",
       "767  [(Facebook, 0.08333333333333333), (Wikipedia, ...   \n",
       "768  [(Facebook, 0.09722222222222221), (Twitter, 0....   \n",
       "769  [(Twitter, 0.0884837333779376), (Wikipedia, 0....   \n",
       "770  [(Wikipedia, 0.06962025316455696), (Facebook, ...   \n",
       "\n",
       "                                            engagement  \\\n",
       "0    [(Blogs, 0.0), (Reddit, 0.14545454545454545), ...   \n",
       "1    [(Videos, 0.16666666666666666), (Mendeley, 0.0...   \n",
       "2    [(Mendeley, 0.0), (Reddit, 0.09090909090909091...   \n",
       "3    [(Reddit, 0.1090909090909091), (Videos, 0.1666...   \n",
       "4    [(Blogs, 0.0), (Reddit, 0.09090909090909091), ...   \n",
       "..                                                 ...   \n",
       "766  [(Reddit, 0.07272727272727272), (Videos, 0.0),...   \n",
       "767  [(Videos, 0.0), (Blogs, 0.0), (Reddit, 0.07272...   \n",
       "768  [(Reddit, 0.10909090909090909), (Mendeley, 0.0...   \n",
       "769  [(Reddit, 0.09090909090909091), (Blogs, 0.0), ...   \n",
       "770  [(Mendeley, 0.0), (Reddit, 0.07272727272727272...   \n",
       "\n",
       "                          feedback  \\\n",
       "0    [(Expert, 0.0), (Peers, 0.0)]   \n",
       "1    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "2    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "3    [(Expert, 0.0), (Peers, 0.0)]   \n",
       "4    [(Expert, 0.0), (Peers, 0.0)]   \n",
       "..                             ...   \n",
       "766  [(Expert, 0.0), (Peers, 0.0)]   \n",
       "767  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "768  [(Expert, 0.0), (Peers, 0.0)]   \n",
       "769  [(Expert, 0.0), (Peers, 0.0)]   \n",
       "770  [(Expert, 0.0), (Peers, 0.0)]   \n",
       "\n",
       "                                             influence  \\\n",
       "0    [(Citation_crossref, 0.14096521945183116), (Ne...   \n",
       "1    [(Citation_crossref, 0.1498249124562281), (New...   \n",
       "2    [(Citation_crossref, 0.12760327532187146), (Ne...   \n",
       "3    [(News, 0.0), (Citation_crossref, 0.1301703483...   \n",
       "4    [(News, 0.0), (Citation_crossref, 0.1256549327...   \n",
       "..                                                 ...   \n",
       "766  [(Citation_crossref, 0.11778257549827545), (Ne...   \n",
       "767  [(Citation_crossref, 0.10234064400621364), (Ne...   \n",
       "768  [(Citation_crossref, 0.12229799110081356), (Ne...   \n",
       "769  [(Citation_crossref, 0.12074458281772465), (Ne...   \n",
       "770  [(Citation_crossref, 0.10288038756220215), (Ne...   \n",
       "\n",
       "                                             outcome  \n",
       "0    [(Patents, 0.07159904534606205), (Policy, 0.0)]  \n",
       "1    [(Patents, 0.07995226730310262), (Policy, 0.0)]  \n",
       "2    [(Policy, 0.0), (Patents, 0.06205250596658711)]  \n",
       "3    [(Patents, 0.06563245823389022), (Policy, 0.0)]  \n",
       "4    [(Patents, 0.06205250596658711), (Policy, 0.0)]  \n",
       "..                                               ...  \n",
       "766  [(Policy, 0.0), (Patents, 0.08591885441527446)]  \n",
       "767  [(Patents, 0.06921241050119331), (Policy, 0.0)]  \n",
       "768  [(Policy, 0.0), (Patents, 0.07875894988066826)]  \n",
       "769  [(Policy, 0.0), (Patents, 0.09069212410501193)]  \n",
       "770  [(Patents, 0.06921241050119331), (Policy, 0.0)]  \n",
       "\n",
       "[771 rows x 10 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_label(node):\n",
    "    label = g.value(node, RDFS.label)\n",
    "    return str(label) if label else str(node)\n",
    "\n",
    "def extract_scores(node):\n",
    "    return [\n",
    "        (str(platform), float(g.value(node, FLOW.score)))\n",
    "        for platform in g.objects(node, FLOW.platform)\n",
    "        if g.value(node, FLOW.score) is not None\n",
    "    ]\n",
    "\n",
    "records = []\n",
    "\n",
    "for pair in g.subjects(RDF.type, FLOW.TopicPair):\n",
    "    topics = list(g.objects(pair, FLOW.hasTopic))\n",
    "    if len(topics) != 2:\n",
    "        continue  # Skip if not exactly two topics\n",
    "    t1, t2 = topics\n",
    "    t1_label, t2_label = get_label(t1), get_label(t2)\n",
    "\n",
    "    shared_concepts = [str(c) for c in g.objects(pair, FLOW.hasSharedConcept)]\n",
    "\n",
    "    def get_impact(prop):\n",
    "        return [x for node in g.objects(pair, prop) for x in extract_scores(node)]\n",
    "\n",
    "    records.append({\n",
    "        \"topic_1_id\": str(t1),\n",
    "        \"topic_1_name\": t1_label,\n",
    "        \"topic_2_id\": str(t2),\n",
    "        \"topic_2_name\": t2_label,\n",
    "        \"shared_concepts\": shared_concepts,\n",
    "        \"reach\": get_impact(FLOW.hasReachImpact),\n",
    "        \"engagement\": get_impact(FLOW.hasEngagementImpact),\n",
    "        \"feedback\": get_impact(FLOW.hasFeedbackImpact),\n",
    "        \"influence\": get_impact(FLOW.hasInfluenceImpact),\n",
    "        \"outcome\": get_impact(FLOW.hasOutcomeImpact)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220fe1f-9fea-4114-b536-9c1930a3d9e3",
   "metadata": {},
   "source": [
    "### 1.1 Model Selection and Zero-Shot Setup\n",
    "\n",
    "This project adopts a train-free evaluation protocol, leveraging state-of-the-art large language models (LLMs) to perform two tasks in a zero-shot setting:  \n",
    "(1) predicting impact stages from structured inputs, and  \n",
    "(2) generating natural language summaries of societal research impact.\n",
    "\n",
    "Traditional machine learning approaches are not suitable for this setting, as they require supervised training data and lack generative capabilities. Instead, we rely on pre-trained LLMs capable of performing classification and data-to-text generation directly from structured prompts.\n",
    "\n",
    "We evaluate seven LLMs spanning both open-source and proprietary model families. These models were selected based on their widespread use, architectural diversity, and ability to handle structured input within a unified inference pipeline. Each model is queried using the same prompt template to ensure consistency in reasoning and output formatting.\n",
    "\n",
    "| Model                | Type         | Version                        | Context Window |\n",
    "|---------------------|--------------|--------------------------------|----------------|\n",
    "| DeepSeek-V3         | Open         | deepseek-chat (v3-base)        | 128k           |\n",
    "| Mixtral 8x22B       | Open         | open-mixtral-8x22b             | 64k            |\n",
    "| GPT-3.5-Turbo       | Proprietary  | gpt-3.5-turbo-0125             | 16k            |\n",
    "| GPT-4o              | Proprietary  | gpt-4o-2024-05-01              | 128k           |\n",
    "| GPT-4o-mini         | Proprietary  | gpt-4o-mini-2024-05-01         | 128k           |\n",
    "| Claude 3.7 Sonnet   | Proprietary  | claude-3-7-sonnet-20250219     | 200k           |\n",
    "| Claude 3.5 Haiku    | Proprietary  | claude-3-5-haiku-20241022      | 200k           |\n",
    "\n",
    "All models are accessed through a unified interface and evaluated under the same pipeline conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9f501-72ce-434e-8921-75af0493b99c",
   "metadata": {},
   "source": [
    "#### 1.1.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7a98204f-05d1-49af-bbe8-b1aca8b89740",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "20caeed1-193f-49ef-82ed-e2a071466815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str, temperature: float = 0.1, max_tokens: int = 4096):\n",
    "    \"\"\"\n",
    "    Loads a local or API-based model using LangChain-compatible interface.\n",
    "    \"\"\"\n",
    "    # Local models\n",
    "    local_model_map = {\n",
    "        \"deepseek\": MODEL_PATH / \"DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"llama\": MODEL_PATH / \"Llama-3.2-1B\",\n",
    "        \"mistral\": MODEL_PATH / \"Mistral-7B-Instruct-v0.1\"\n",
    "    }\n",
    "\n",
    "    if model_name in local_model_map:\n",
    "        model_path = local_model_map[model_name]\n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=str(model_path),\n",
    "            device=\"cuda\",\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens\n",
    "        )\n",
    "        return HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "    # OpenAI models\n",
    "    elif model_name in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-instruct\", \"gpt-4\", \"gpt-4o\", \"gpt-4o-mini\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise EnvironmentError(\"OPENAI_API_KEY not set in environment variables.\")\n",
    "        return ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "\n",
    "    # Anthropic models (Claude)\n",
    "    elif model_name in [\"claude-3-7-sonnet-20250219\", \"claude-3-5-haiku-20241022\"]:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise EnvironmentError(\"ANTHROPIC_API_KEY not set in environment variables.\")\n",
    "        return ChatAnthropic(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            anthropic_api_key=api_key\n",
    "        )\n",
    "    # DeepSeek API via OpenAI-compatible interface\n",
    "    elif model_name in [\"deepseek-reasoner\", \"deepseek-chat\"]:\n",
    "        api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise EnvironmentError(\"DEEPSEEK_API_KEY not set in environment variables.\")\n",
    "        return ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            openai_api_key=api_key,\n",
    "            openai_api_base=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "     # Mistral API (Mistral Large or similar)\n",
    "    elif model_name in [\"open-mixtral-8x22b\", \"open-mixtral-8x7b\"]:\n",
    "        api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise EnvironmentError(\"MISTRAL_API_KEY not set in environment variables.\")\n",
    "        return ChatMistralAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            mistral_api_key=api_key\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae24d-4554-4fab-b852-1a008eeb115c",
   "metadata": {},
   "source": [
    "#### 1.1.2 Prompt Templates for Zero-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "033241f8-7ea6-4cee-b3dc-81355219f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_name: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Generates a response from a local or API-based model via LLMChain.\n",
    "    \"\"\"\n",
    "    model = load_model(model_name)\n",
    "\n",
    "    if isinstance(model, HuggingFacePipeline):\n",
    "        template = PromptTemplate(template=\"{prompt}\", input_variables=[\"prompt\"])\n",
    "    else:\n",
    "        template = PromptTemplate(template=\"{prompt}\", input_variables=[\"prompt\"])\n",
    "\n",
    "    llm_chain = LLMChain(llm=model, prompt=template)\n",
    "\n",
    "    return llm_chain.run({\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f96ac753-4c7f-4185-89df-a9ebffda0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prompt_template(row):\n",
    "    \"\"\"\n",
    "    Generate a prompt to classify the joint impact stage of a research topic pair,\n",
    "    using structured platform co-mention evidence and shared concepts.\n",
    "    The platform values are normalised per platform within each dimension.\n",
    "    \"\"\"\n",
    "    topic_1 = row[\"topic_1_name\"]\n",
    "    topic_2 = row[\"topic_2_name\"]\n",
    "    shared_concepts = ', '.join(row[\"shared_concepts\"]) or \"None\"\n",
    "\n",
    "    platform_lines = []\n",
    "    for dim in impact_stages:\n",
    "        entries = row.get(dim, [])\n",
    "    \n",
    "        if isinstance(entries, str):\n",
    "            try:\n",
    "                entries = ast.literal_eval(entries)\n",
    "            except Exception:\n",
    "                entries = []\n",
    "    \n",
    "        if isinstance(entries, list):\n",
    "            dim_totals = defaultdict(float)\n",
    "            for item in entries:\n",
    "                if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "                    platform, value = item\n",
    "                    try:\n",
    "                        dim_totals[platform] += float(value)\n",
    "                    except Exception:\n",
    "                        continue  # Skip if value cannot be converted to float\n",
    "            platform_info = ', '.join(\n",
    "                f\"{platform}: {round(value, 3)}\" for platform, value in dim_totals.items()\n",
    "            )\n",
    "            platform_lines.append(f\"{platform_info if platform_info else 'None'}\")\n",
    "    \n",
    "    full_platform = '\\n'.join(platform_lines)\n",
    "    pairs = [p.strip() for line in full_platform.splitlines() for p in line.split(\",\")]\n",
    "    full_platform_lines = \", \".join(pairs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            You are an expert in research impact analysis. Your task is to assess the impact of a pair of research topics based on structured evidence of platform co-mentions and shared concepts.\n",
    "            \n",
    "            This is a multi-label classification problem. Your goal is to classify all impact stages as either supported or not, based on the strength and relevance of the evidence for each stage.\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            Impact Stages:\n",
    "            \n",
    "            - Reach: Broad dissemination of research to general audiences via mass communication platforms (e.g., Twitter, Facebook, Wikipedia).\n",
    "            - Engagement: Active interaction, discussion, or interpretation of research in community-driven forums (e.g., Blogs, Reddit, YouTube, Mendeley).\n",
    "            - Feedback: Scholarly reactions or critical appraisals, often indicating academic interest (e.g., Peer Review).\n",
    "            - Influence: Contribution to discourse in authoritative contexts (e.g., citations via CrossRef, media coverage via News).\n",
    "            - Outcome: Tangible societal or technological effects arising from research (e.g., Policy documents, Patents).\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            Important Notes:\n",
    "            \n",
    "            - Platform values are normalised per platform and impact dimension at the topic level (0.0 = no evidence, 1.0 = maximum evidence). For topic pairs, scores are summed (range: 0.0–2.0) to reflect combined impact.\n",
    "            - Classify all impact stages with meaningful or emerging support, considering the total cumulative evidence across platforms, even if individual platform scores are low.\n",
    "            - Err slightly on the side of inclusiveness: if multiple signals exist across platforms, prefer assigning the stage rather than omitting it.\n",
    "            - If cumulative evidence across all platforms for a stage is strictly zero, assign only the \"Reach\" stage.\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            Now classify this pair:\n",
    "            \n",
    "            - Topic 1: {topic_1}  \n",
    "            - Topic 2: {topic_2}  \n",
    "            - Shared Concepts: {shared_concepts}\n",
    "            \n",
    "            Platform Co-mention Evidence (normalised values):  \n",
    "            {full_platform_lines}\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            You must begin your output exactly with:\n",
    "            \n",
    "            Impact Stages with Sufficient Support: [list of stages]\n",
    "            \n",
    "            Followed by:\n",
    "            \n",
    "            Impact Summary:\n",
    "            Write a concise (2–4 sentence) summary explaining the evidence for each assigned impact stage. Mention the key platforms and shared concepts that support each stage. Highlight how cumulative evidence across platforms contributed to classification, even if individual signals are small.\n",
    "            \"\"\"\n",
    "\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d38e788b-da90-472d-a622-02bbec0d2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_and_generate_responses(topic_pairs_df, model_name):\n",
    "    print(f\"Running for model: {model_name}\")\n",
    "    \n",
    "    impact_stages_set = {\"reach\", \"engagement\", \"feedback\", \"influence\", \"outcome\"}\n",
    "\n",
    "    predicted_stages = []\n",
    "    impact_summaries = []\n",
    "    raw_responses = []\n",
    "\n",
    "    for i in range(len(topic_pairs_df)):\n",
    "        row = topic_pairs_df.iloc[i]\n",
    "\n",
    "        # Generate prompt\n",
    "        prompt = apply_prompt_template(row)\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(model_name, prompt).strip()\n",
    "        raw_responses.append(response)\n",
    "\n",
    "        # Extract supported stages\n",
    "        stages_match = re.search(r\"Impact Stages with Sufficient Support:\\s*\\[([^\\]]+)\\]\", response)\n",
    "        if stages_match:\n",
    "            supported_stages = stages_match.group(1).split(\",\")\n",
    "            supported_stages = [stage.strip().lower() for stage in supported_stages if stage.strip()]\n",
    "        else:\n",
    "            supported_stages = []\n",
    "\n",
    "        supported_stages = [stage for stage in supported_stages if stage in impact_stages_set]\n",
    "        predicted_stages.append(supported_stages)\n",
    "\n",
    "        # Extract impact summary\n",
    "        summary_match = re.search(r\"Impact Summary:\\s*(.+)\", response, re.DOTALL)\n",
    "        summary = summary_match.group(1).strip() if summary_match else \"\"\n",
    "        impact_summaries.append(summary)\n",
    "\n",
    "    # Assign new columns\n",
    "    topic_pairs_df[f\"predicted_stages_{model_name}\"] = predicted_stages\n",
    "    topic_pairs_df[f\"impact_summary_{model_name}\"] = impact_summaries\n",
    "    topic_pairs_df[f\"raw_response_{model_name}\"] = raw_responses\n",
    "\n",
    "    return topic_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e6c8b-cac2-4664-876c-2c427b29c6c0",
   "metadata": {},
   "source": [
    "### 1.2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8d0e5c50-b217-442e-9df4-d6a8a0caac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for model: open-mixtral-8x22b\n",
      "Running for model: deepseek-chat\n",
      "Running for model: gpt-3.5-turbo\n",
      "Running for model: gpt-4o\n",
      "Running for model: gpt-4o-mini\n",
      "Running for model: claude-3-7-sonnet-20250219\n",
      "Running for model: claude-3-5-haiku-20241022\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    topic_pairs_df = iterate_and_generate_responses(df, model_name=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b4f07c5a-480c-4d87-97c7-11f0f1ec4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pre-processed files\n",
    "DATASET_PATH = project_dir / \"data\" / \"flowmetrics-predictions.csv\"\n",
    "topic_pairs_df.to_csv(DATASET_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

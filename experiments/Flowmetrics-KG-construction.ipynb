{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc3ac46-9cdd-457a-b496-f3127edc2250",
   "metadata": {},
   "source": [
    "# Flowmetrics – Impact-Augmented Knowledge Graph Construction\n",
    "\n",
    "This notebook constructs the Impact-Augmented Knowledge Graph, the central data structure underpinning the Flowmetrics framework. It documents the end-to-end pipeline for generating a structured dataset of societal research impact trajectories by integrating heterogeneous data sources into an RDF graph suitable for AI-driven impact modelling.\n",
    "\n",
    "### Objective\n",
    "\n",
    "To automate the extraction, semantic alignment, and structuring of research topic pairs and their associated impact signals — enabling scalable modelling of how research impact unfolds across time, platforms, and audiences.\n",
    "\n",
    "### Structure\n",
    "\n",
    "#### 1. Data Collection Pipeline  \n",
    "Harvests impact evidence through API integration with three key platforms:  \n",
    "- **arXiv** – source of metadata for topic extraction and co-occurrence modelling  \n",
    "- **Altmetric** – provider of online attention signals (e.g., news, social media, blogs)  \n",
    "- **CrossRef** – supplier of citation-based and policy-linked influence metrics\n",
    "\n",
    "The pipeline operates on a curated corpus of 12,350 computer science preprints (2000–2024), spanning major subfields such as machine learning, natural language processing, computer vision, and artificial intelligence. Papers were selected for topical diversity, metadata completeness, and coverage across at least one impact platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11db9d-524c-4f8f-a224-fa35af4d68fb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Data collection pipeline: automating data extraction](#section-1)\n",
    "  - [1.1 API Integration](#subsection-11)\n",
    "     - [1.1.1 Altmetric Data](#subsection-111)\n",
    "     - [1.1.2 CrossRef Data](#subsection-112)\n",
    "     - [1.1.3 arXiv Data](#subsection-113)\n",
    "  - [1.2 Impact Trajectory Matching](#subsection-12)\n",
    "     - [1.2.1 Nodes of topics](#subsection-121)\n",
    "     - [1.2.2 Edges of impact](#subsection-122)\n",
    "     - [1.2.3 Impact-augmented knowledge graph](#subsection-123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cd920f-bd51-4f1c-b084-b652a5f76b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from cso_classifier import CSOClassifier\n",
    "from rdflib import Graph as RDFGraph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from config import models, impact_stages, stage_order\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5401fc-0fb7-4b33-aeb5-11450e20571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path(\".\").resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab467c9e-bd29-4c38-b3bf-d1355b57cffc",
   "metadata": {},
   "source": [
    "## 1.1 API Integration  \n",
    "### 1.1.1 Altmetric Data\n",
    "\n",
    "Altmetric data captures the online attention and engagement surrounding scholarly publications, providing insight into the broader impact of research beyond traditional citation metrics. It reflects how a paper is being discussed, shared, and interacted with across a range of platforms including news outlets, blogs, Twitter, Facebook, Reddit, Wikipedia, and policy documents.\n",
    "\n",
    "This data helps construct a multidimensional view of research visibility and societal relevance — spanning public discourse, academic engagement, and policy uptake. As such, Altmetric indicators are increasingly valuable for researchers, institutions, and funders aiming to understand and quantify how research resonates across different audiences and sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790c3b86-c2d9-4fa3-b4cb-f69023edf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altmetric_data(doi):\n",
    "    url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        altmetric_score = data.get(\"score\", 0)\n",
    "        all_mentions_counts = data.get(\"cited_by_posts_count\", 0)\n",
    "        twitter_counts = data.get(\"cited_by_tweeters_count\", 0)\n",
    "        news_counts = data.get(\"cited_by_news_outlets_count\", 0)\n",
    "        blogs_counts = data.get(\"cited_by_blogs_count\", 0)\n",
    "        reddit_counts = data.get(\"cited_by_rdts_count\", 0)\n",
    "        facebook_counts = data.get(\"cited_by_fbwalls_count\", 0)\n",
    "        patents_counts = data.get(\"cited_by_patents_count\", 0)\n",
    "        wiki_counts = data.get(\"cited_by_wikipedia_count\", 0)\n",
    "        policy_counts = data.get(\"cited_by_policy_count\", 0)\n",
    "        mendeley_counts = data.get(\"cited_by_mendeley_count\", 0)\n",
    "        video_counts = data.get(\"cited_by_videos_count\", 0)\n",
    "        return altmetric_score, all_mentions_counts, twitter_counts, news_counts, blogs_counts, reddit_counts, facebook_counts, patents_counts, wiki_counts, policy_counts, mendeley_counts, video_counts\n",
    "    elif response.status_code == 401:\n",
    "        print(\"Unauthorized: Check your API key or permissions.\")\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Rate limit exceeded. Please try again later.\")\n",
    "    else:\n",
    "        pass\n",
    "    return None, None, None, None, None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261a740-a68b-4f5a-ad7e-7f50038d784a",
   "metadata": {},
   "source": [
    "### 1.1.2 CrossRef Data\n",
    "\n",
    "CrossRef data provides extensive metadata and citation information for scholarly works, including journal articles, books, conference proceedings, datasets, and other research outputs. It plays a critical role in enhancing the discoverability and traceability of academic content through the use of persistent identifiers such as Digital Object Identifiers (DOIs).\n",
    "\n",
    "Within the Flowmetrics framework, CrossRef serves as a key source of citation-based impact signal. These signals help trace the academic and institutional reach of research over time, offering a complementary view to socially-driven metrics and enabling a more comprehensive understanding of scholarly influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a753e3ce-70c8-4c23-91b8-09a132f7a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_count_from_crossref(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        citation_count = data.get('message', {}).get('is-referenced-by-count', 0)\n",
    "        return citation_count\n",
    "    elif response.status_code == 404:\n",
    "        pass\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Rate limit exceeded. Please try again later.\")\n",
    "    else:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a980bca-00f4-448b-b8ac-7f4d8ab396c7",
   "metadata": {},
   "source": [
    "### 1.1.3 arXiv Data\n",
    "\n",
    "arXiv is a widely used preprint repository that offers open access to scholarly articles across a broad range of disciplines, including computer science, physics, mathematics, statistics, electrical engineering, quantitative biology, and economics.\n",
    "\n",
    "In the Flowmetrics framework, arXiv serves as the primary source for research metadata, enabling the extraction of topics and co-occurrence patterns. This metadata provides the structural backbone for identifying topic pairs and aligning them with downstream impact signals collected from Altmetric and CrossRef.\n",
    "\n",
    "Download the Dataset: The arXiv snapshot utilised in this project is available for download from Kaggle (https://www.kaggle.com/datasets/Cornell-University/arxiv), provided by Cornell University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e09970-b342-4511-94be-3fb99d35fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = project_dir / \"data\" / \"arxiv-metadata-oai-snapshot.json\" # Download here: https://www.kaggle.com/datasets/Cornell-University/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26a388b-aba9-4f1e-b22a-26e313f40d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {\n",
    "    'cs.AI': 'Artificial Intelligence',\n",
    "    'cs.CL': 'Computation and Language',\n",
    "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "    'cs.DS': 'Data Structures and Algorithms',\n",
    "    'cs.ET': 'Emerging Technologies',\n",
    "    'cs.HC': 'Human-Computer Interaction',\n",
    "    'cs.IR': 'Information Retrieval',\n",
    "    'cs.NE': 'Neural and Evolutionary Computing',\n",
    "    'cs.LG': 'Machine Learning'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1451024-1617-4ded-b4f3-6f86f39fbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return ' '.join(filtered_sentence).replace(',','')\n",
    "\n",
    "def lemmatizer_word(word):\n",
    "    return wnl.lemmatize(word)\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer_word(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def get_metadata():\n",
    "    with open(DATASET_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f55930-6c25-48cf-bf10-6605824a4513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12536, 12536, 12536, 12536, 12536, 12536, 12536)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dois = []\n",
    "titles = []\n",
    "abstracts = []\n",
    "years = []\n",
    "categories = []\n",
    "citations_crossref = []\n",
    "citations_wos = []\n",
    "altmetric_score = []\n",
    "all_mentions_counts = []\n",
    "twitter_counts = []\n",
    "news_counts = []\n",
    "blogs_counts = []\n",
    "reddit_counts = []\n",
    "facebook_counts = []\n",
    "patents_counts = []\n",
    "wiki_counts = []\n",
    "policy_counts = []\n",
    "mendeley_counts = []\n",
    "video_counts = []\n",
    "metadata = get_metadata()\n",
    "count = 0\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    ref = paper_dict.get('journal-ref')\n",
    "    doi = paper_dict.get('doi')\n",
    "    try:\n",
    "        year = int(ref[-4:]) \n",
    "        if 2000 < year <= 2024:\n",
    "            count+=1\n",
    "            categories.append(category_map[paper_dict.get('categories').split(\" \")[0]])\n",
    "            years.append(year)\n",
    "            titles.append(lemmatizer(remove_stop_words(paper_dict.get('title'))))\n",
    "            abstracts.append(lemmatizer(remove_stop_words(paper_dict.get('abstract'))))\n",
    "            dois.append(doi)\n",
    "            citations_crossref.append(get_citation_count_from_crossref(doi))\n",
    "            a, b, c, d, e, f, g, h, i, j, k, l = get_altmetric_data(doi)\n",
    "            altmetric_score.append(a)\n",
    "            all_mentions_counts.append(b)\n",
    "            twitter_counts.append(c)\n",
    "            news_counts.append(d)\n",
    "            blogs_counts.append(e)\n",
    "            reddit_counts.append(f)\n",
    "            facebook_counts.append(g)\n",
    "            patents_counts.append(h)\n",
    "            wiki_counts.append(i)\n",
    "            policy_counts.append(j)\n",
    "            mendeley_counts.append(k)\n",
    "            video_counts.append(l)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(dois), len(titles), len(abstracts), len(years), len(categories), len(citations_crossref), len(altmetric_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55375943-b3a8-4041-826a-33eefebdcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'DOI': dois,\n",
    "    'Title': titles,\n",
    "    'Abstract': abstracts,\n",
    "    'Year': years,\n",
    "    'Category': categories,\n",
    "    'Citation_crossref': citations_crossref,\n",
    "    'Altmetric_score': altmetric_score,\n",
    "    'All_mentions': all_mentions_counts,\n",
    "    'Twitter': twitter_counts,\n",
    "    'News': news_counts,\n",
    "    'Blogs': blogs_counts,\n",
    "    'Reddit': reddit_counts,\n",
    "    'Facebook': facebook_counts,\n",
    "    'Patents': patents_counts,\n",
    "    'Policy': policy_counts,\n",
    "    'Mendeley': mendeley_counts,\n",
    "    'Wikipedia': wiki_counts,\n",
    "    'Videos': video_counts,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a8fa9-34fe-4298-baf1-d1a381a7fc33",
   "metadata": {},
   "source": [
    "## 1.2 Impact Trajectory Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2060e755-f252-414d-baeb-6e69c500ff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Category</th>\n",
       "      <th>Citation_crossref</th>\n",
       "      <th>Altmetric_score</th>\n",
       "      <th>All_mentions</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>News</th>\n",
       "      <th>Blogs</th>\n",
       "      <th>Reddit</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Patents</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Mendeley</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.comgeo.2008.05.005</td>\n",
       "      <td>Edges Switches Tunnels Bridges</td>\n",
       "      <td>Edge casing well-known method improve readabil...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.tcs.2008.03.029</td>\n",
       "      <td>Soft constraint abstraction based semiring hom...</td>\n",
       "      <td>The semiring-based constraint satisfaction pro...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1093/comjnl/bxm084</td>\n",
       "      <td>On Ultrametric Algorithmic Information</td>\n",
       "      <td>How best quantify information object whether n...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1007/978-3-540-78568-2</td>\n",
       "      <td>Efficient Algorithms Node Disjoint Subgraph Ho...</td>\n",
       "      <td>Recently great effort dedicated research manag...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.2478/s13230-010-0014-0</td>\n",
       "      <td>Toward Psycho-robots</td>\n",
       "      <td>We try perform geometrization psychology repre...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505</th>\n",
       "      <td>10.1109/TED.2007.893191</td>\n",
       "      <td>Retraction Generalized Extension Computing Words</td>\n",
       "      <td>Fuzzy automaton whose input alphabet set numbe...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12509</th>\n",
       "      <td>10.1007/s00357-007-0007-9</td>\n",
       "      <td>The Haar Wavelet Transform Dendrogram</td>\n",
       "      <td>We describe new wavelet transform use hierarch...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Information Retrieval</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12527</th>\n",
       "      <td>10.1093/comjnl/bxl065</td>\n",
       "      <td>Hedging prediction machine learning</td>\n",
       "      <td>Recent advance machine learning make possible ...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>87.0</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12528</th>\n",
       "      <td>10.1142/S1793005708001100</td>\n",
       "      <td>A Neutrosophic Description Logic</td>\n",
       "      <td>Description Logics ( DLs ) appropriate widely ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>10.1137/S0097539700370084</td>\n",
       "      <td>Setting Parameters Example</td>\n",
       "      <td>We introduce class `` inverse parametric optim...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6186 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DOI  \\\n",
       "3      10.1016/j.comgeo.2008.05.005   \n",
       "4         10.1016/j.tcs.2008.03.029   \n",
       "10            10.1093/comjnl/bxm084   \n",
       "14        10.1007/978-3-540-78568-2   \n",
       "15        10.2478/s13230-010-0014-0   \n",
       "...                             ...   \n",
       "12505       10.1109/TED.2007.893191   \n",
       "12509     10.1007/s00357-007-0007-9   \n",
       "12527         10.1093/comjnl/bxl065   \n",
       "12528     10.1142/S1793005708001100   \n",
       "12535     10.1137/S0097539700370084   \n",
       "\n",
       "                                                   Title  \\\n",
       "3                         Edges Switches Tunnels Bridges   \n",
       "4      Soft constraint abstraction based semiring hom...   \n",
       "10                On Ultrametric Algorithmic Information   \n",
       "14     Efficient Algorithms Node Disjoint Subgraph Ho...   \n",
       "15                                  Toward Psycho-robots   \n",
       "...                                                  ...   \n",
       "12505   Retraction Generalized Extension Computing Words   \n",
       "12509              The Haar Wavelet Transform Dendrogram   \n",
       "12527                Hedging prediction machine learning   \n",
       "12528                   A Neutrosophic Description Logic   \n",
       "12535                         Setting Parameters Example   \n",
       "\n",
       "                                                Abstract  Year  \\\n",
       "3      Edge casing well-known method improve readabil...  2009   \n",
       "4      The semiring-based constraint satisfaction pro...  2008   \n",
       "10     How best quantify information object whether n...  2010   \n",
       "14     Recently great effort dedicated research manag...  2008   \n",
       "15     We try perform geometrization psychology repre...  2010   \n",
       "...                                                  ...   ...   \n",
       "12505  Fuzzy automaton whose input alphabet set numbe...  2007   \n",
       "12509  We describe new wavelet transform use hierarch...  2007   \n",
       "12527  Recent advance machine learning make possible ...  2007   \n",
       "12528  Description Logics ( DLs ) appropriate widely ...  2006   \n",
       "12535  We introduce class `` inverse parametric optim...  2003   \n",
       "\n",
       "                             Category  Citation_crossref  Altmetric_score  \\\n",
       "3      Data Structures and Algorithms                7.0            5.488   \n",
       "4             Artificial Intelligence                6.0              NaN   \n",
       "10            Artificial Intelligence                9.0              NaN   \n",
       "14     Data Structures and Algorithms                0.0              NaN   \n",
       "15            Artificial Intelligence                1.0              NaN   \n",
       "...                               ...                ...              ...   \n",
       "12505         Artificial Intelligence                4.0              NaN   \n",
       "12509           Information Retrieval               48.0              NaN   \n",
       "12527                Machine Learning               87.0            4.000   \n",
       "12528         Artificial Intelligence                0.0              NaN   \n",
       "12535  Data Structures and Algorithms               18.0            5.488   \n",
       "\n",
       "       All_mentions  Twitter  News  Blogs  Reddit  Facebook  Patents  Policy  \\\n",
       "3               2.0      0.0   0.0    0.0     0.0       0.0      0.0     0.0   \n",
       "4               NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "10              NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "14              NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "15              NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "...             ...      ...   ...    ...     ...       ...      ...     ...   \n",
       "12505           NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "12509           NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "12527           2.0      1.0   0.0    0.0     0.0       0.0      0.0     0.0   \n",
       "12528           NaN      NaN   NaN    NaN     NaN       NaN      NaN     NaN   \n",
       "12535           2.0      0.0   0.0    0.0     0.0       0.0      0.0     0.0   \n",
       "\n",
       "       Mendeley  Wikipedia  Videos  \n",
       "3           0.0        0.0     0.0  \n",
       "4           NaN        NaN     NaN  \n",
       "10          NaN        NaN     NaN  \n",
       "14          NaN        NaN     NaN  \n",
       "15          NaN        NaN     NaN  \n",
       "...         ...        ...     ...  \n",
       "12505       NaN        NaN     NaN  \n",
       "12509       NaN        NaN     NaN  \n",
       "12527       0.0        1.0     0.0  \n",
       "12528       NaN        NaN     NaN  \n",
       "12535       0.0        0.0     0.0  \n",
       "\n",
       "[6186 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['DOI'].notna()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa2f365-4dc4-40a9-8e4a-76deca699b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Computer Vision and Pattern Recognition    1976\n",
       "Machine Learning                           1453\n",
       "Artificial Intelligence                     809\n",
       "Computation and Language                    618\n",
       "Human-Computer Interaction                  404\n",
       "Data Structures and Algorithms              296\n",
       "Neural and Evolutionary Computing           273\n",
       "Information Retrieval                       269\n",
       "Emerging Technologies                        88\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3deea-79f0-4d91-929d-9438bea7cd3c",
   "metadata": {},
   "source": [
    "### 1.2.1 Nodes of Topics\n",
    "\n",
    "The nodes of the knowledge graph represent high-quality research topics extracted from arXiv metadata. To identify these topics, we used the **CSO Classifier** — an unsupervised tool that assigns concepts from the **Computer Science Ontology (CSO)** based on paper titles, abstracts, and keywords.\n",
    "\n",
    "The CSO Classifier integrates two components:  \n",
    "- A **syntactic module**, which detects explicitly mentioned concepts  \n",
    "- A **semantic module**, which leverages part-of-speech tagging and word embeddings to infer related concepts\n",
    "\n",
    "Outputs from both modules are merged to generate a candidate topic list for each paper. To consolidate fine-grained variations, we applied a **frequency-based clustering strategy**, assigning each paper to its most frequently associated concept across the corpus. This allowed for the grouping of semantically similar papers under unified topic identifiers without manual filtering.\n",
    "\n",
    "The resulting pipeline generated **156 coherent, domain-relevant topic nodes**, covering diverse areas of computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413ea1a8-2013-4bae-a2f7-ade5d4c9f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CSOClassifier(modules = \"both\", enhancement = \"all\", explanation = False)\n",
    "results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33e0d36-7812-40a4-b90e-863dd33a15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_dict(row):\n",
    "    paper = {\n",
    "        \"title\": row[\"Title\"],\n",
    "        \"abstract\": row[\"Abstract\"]\n",
    "    }\n",
    "    return cc.run(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82873fd-5a19-4e74-b8b5-5b16bdf62d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Apply function to create the 'cso_topics' column\n",
    "df[\"CSO_classifier\"] = df.apply(create_paper_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c37c138c-e7f8-4135-a0e8-439d3f93b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Data Structures and Algorithms\n",
      "Title: Edges Switches Tunnels Bridges\n",
      "Abstract: Edge casing well-known method improve readability drawing non-planar graph . A cased drawing order edge edge crossing interrupt lower edge appropriate neighborhood crossing . Certain order lead readable drawing others . We formulate several optimization criterion try capture concept `` good `` cased drawing . Further address algorithmic question turn given drawing optimal cased drawing . For many resulting optimization problem either find polynomial time algorithm NP-hardness result .\n",
      "CSO Topics: \n",
      "{\n",
      "    \"syntactic\": [\n",
      "        \"optimization problems\",\n",
      "        \"optimization\",\n",
      "        \"polynomial-time algorithms\",\n",
      "        \"edge crossing\"\n",
      "    ],\n",
      "    \"semantic\": [\n",
      "        \"optimization problems\",\n",
      "        \"optimization\",\n",
      "        \"polynomial-time algorithms\",\n",
      "        \"edge point\",\n",
      "        \"planar graph\"\n",
      "    ],\n",
      "    \"union\": [\n",
      "        \"optimization problems\",\n",
      "        \"optimization\",\n",
      "        \"edge crossing\",\n",
      "        \"planar graph\",\n",
      "        \"polynomial-time algorithms\",\n",
      "        \"edge point\"\n",
      "    ],\n",
      "    \"enhanced\": [\n",
      "        \"correlation analysis\",\n",
      "        \"mathematics\",\n",
      "        \"graph drawing\",\n",
      "        \"drawing (graphics)\",\n",
      "        \"graphic methods\",\n",
      "        \"polynomial approximation\",\n",
      "        \"edge detection algorithms\",\n",
      "        \"graph theory\",\n",
      "        \"visualization\",\n",
      "        \"computer aided design\",\n",
      "        \"edge detection\",\n",
      "        \"signal detection\",\n",
      "        \"theoretical computer science\",\n",
      "        \"human computer interaction\",\n",
      "        \"computer science\",\n",
      "        \"image segmentation\",\n",
      "        \"signal processing\",\n",
      "        \"image analysis\",\n",
      "        \"image processing\",\n",
      "        \"engineering\",\n",
      "        \"computer imaging and vision\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Category: Artificial Intelligence\n",
      "Title: Soft constraint abstraction based semiring homomorphism\n",
      "Abstract: The semiring-based constraint satisfaction problem ( semiring CSPs ) proposed Bistarelli Montanari Rossi \\cite { BMR97 } general framework soft constraint . In paper propose abstraction scheme soft constraint us semiring homomorphism . To find optimal solution concrete problem idea first working abstract problem finding optimal solution using solve concrete problem . In particular show mapping preserve optimal solution order-reflecting semiring homomorphism . Moreover semiring homomorphism $ \\alpha $ problem $ P $ $ S $ $ $ optimal $ \\alpha ( P ) $ optimal solution $ \\bar { } $ $ P $ $ \\bar { } $ value $ $ $ \\alpha ( P ) $ .\n",
      "CSO Topics: \n",
      "{\n",
      "    \"syntactic\": [\n",
      "        \"optimal solutions\"\n",
      "    ],\n",
      "    \"semantic\": [\n",
      "        \"combinatorial optimization\",\n",
      "        \"constraint satisfaction problems (csp)\",\n",
      "        \"combinatorial problems\",\n",
      "        \"constraint programming\",\n",
      "        \"constraint networks\"\n",
      "    ],\n",
      "    \"union\": [\n",
      "        \"constraint satisfaction problems (csp)\",\n",
      "        \"combinatorial problems\",\n",
      "        \"constraint programming\",\n",
      "        \"optimal solutions\",\n",
      "        \"combinatorial optimization\",\n",
      "        \"constraint networks\"\n",
      "    ],\n",
      "    \"enhanced\": [\n",
      "        \"artificial intelligence\",\n",
      "        \"combinatorial mathematics\",\n",
      "        \"computer programming\",\n",
      "        \"computer systems\",\n",
      "        \"optimization\",\n",
      "        \"computer science\",\n",
      "        \"graph theory\",\n",
      "        \"mathematics\",\n",
      "        \"theoretical computer science\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Category: Artificial Intelligence\n",
      "Title: On Ultrametric Algorithmic Information\n",
      "Abstract: How best quantify information object whether natural artifact problem wide interest . A related problem computability object . We present practical example new way address problem . By giving appropriate representation object based hierarchical coding information exemplify remarkably easy compute complex object . Our algorithmic complexity related length class object rather length object .\n",
      "CSO Topics: \n",
      "{\n",
      "    \"syntactic\": [\n",
      "        \"computability\"\n",
      "    ],\n",
      "    \"semantic\": [\n",
      "        \"computability\",\n",
      "        \"combinatorial problems\",\n",
      "        \"combinatorial optimization\",\n",
      "        \"design of algorithms\"\n",
      "    ],\n",
      "    \"union\": [\n",
      "        \"combinatorial optimization\",\n",
      "        \"combinatorial problems\",\n",
      "        \"computability\",\n",
      "        \"design of algorithms\"\n",
      "    ],\n",
      "    \"enhanced\": [\n",
      "        \"optimization\",\n",
      "        \"combinatorial mathematics\",\n",
      "        \"computability and decidability\",\n",
      "        \"turing machines\",\n",
      "        \"mathematics\",\n",
      "        \"graph theory\",\n",
      "        \"formal logic\",\n",
      "        \"automata theory\",\n",
      "        \"formal languages\",\n",
      "        \"theoretical computer science\",\n",
      "        \"artificial intelligence\",\n",
      "        \"formal languages and automata theory\",\n",
      "        \"linguistics\",\n",
      "        \"computer science\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.head(3).iterrows():\n",
    "    print(f\"Category: {row['Category']}\")\n",
    "    print(f\"Title: {row['Title']}\")\n",
    "    print(f\"Abstract: {row['Abstract']}\")\n",
    "    print(f\"CSO Topics: \\n{json.dumps(row['CSO_classifier'], indent=4)}\\n\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d06ec6-b228-460f-99ab-87036aeb07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"union\" list from each dictionary\n",
    "df[\"Research_concepts\"] = df[\"CSO_classifier\"].apply(lambda x: x.get(\"union\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9bfa89-0103-4916-a71a-d3bb776b0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Super_topics\"] = df[\"CSO_classifier\"].apply(lambda x: x.get(\"enhanced\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217dae7f-415b-42d9-a7e2-703aa6948e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Research_concepts\"] = df[\"Research_concepts\"].apply(ast.literal_eval)\n",
    "all_concepts = df[\"Research_concepts\"].apply(lambda x: x).explode().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9297974f-1106-4712-a4d9-2348a0e03b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the frequency of each concept across all papers\n",
    "concept_counts = pd.Series(all_concepts).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23c1821-ad41-489b-8fd2-f79436f1a34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Category</th>\n",
       "      <th>Citation_crossref</th>\n",
       "      <th>Altmetric_score</th>\n",
       "      <th>All_mentions</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>News</th>\n",
       "      <th>...</th>\n",
       "      <th>Patents</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Mendeley</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Videos</th>\n",
       "      <th>CSO_classifier</th>\n",
       "      <th>Research_concepts</th>\n",
       "      <th>Super_topics</th>\n",
       "      <th>Name</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.comgeo.2008.05.005</td>\n",
       "      <td>Edges Switches Tunnels Bridges</td>\n",
       "      <td>Edge casing well-known method improve readabil...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'syntactic': ['optimization problems', 'optim...</td>\n",
       "      <td>[optimization problems, optimization, edge cro...</td>\n",
       "      <td>[correlation analysis, mathematics, graph draw...</td>\n",
       "      <td>optimization</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.tcs.2008.03.029</td>\n",
       "      <td>Soft constraint abstraction based semiring hom...</td>\n",
       "      <td>The semiring-based constraint satisfaction pro...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['optimal solutions'], 'semantic...</td>\n",
       "      <td>[constraint satisfaction problems (csp), combi...</td>\n",
       "      <td>[artificial intelligence, combinatorial mathem...</td>\n",
       "      <td>combinatorial problems</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1093/comjnl/bxm084</td>\n",
       "      <td>On Ultrametric Algorithmic Information</td>\n",
       "      <td>How best quantify information object whether n...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['computability'], 'semantic': [...</td>\n",
       "      <td>[combinatorial optimization, combinatorial pro...</td>\n",
       "      <td>[optimization, combinatorial mathematics, comp...</td>\n",
       "      <td>combinatorial problems</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1007/978-3-540-78568-2</td>\n",
       "      <td>Efficient Algorithms Node Disjoint Subgraph Ho...</td>\n",
       "      <td>Recently great effort dedicated research manag...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['graph-based', 'synthetic data'...</td>\n",
       "      <td>[graph-based, synthetic data, state space, mat...</td>\n",
       "      <td>[graphic methods, machine learning, state spac...</td>\n",
       "      <td>matching algorithm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.2478/s13230-010-0014-0</td>\n",
       "      <td>Toward Psycho-robots</td>\n",
       "      <td>We try perform geometrization psychology repre...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['cognitive systems', 'robots', ...</td>\n",
       "      <td>[mobile robots, cognitive systems, cognitive s...</td>\n",
       "      <td>[robotics, computer science, topology]</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505</th>\n",
       "      <td>10.1109/TED.2007.893191</td>\n",
       "      <td>Retraction Generalized Extension Computing Words</td>\n",
       "      <td>Fuzzy automaton whose input alphabet set numbe...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['automation', 'feature models',...</td>\n",
       "      <td>[automation, feature models, nondeterministic ...</td>\n",
       "      <td>[engineering, software product line, translati...</td>\n",
       "      <td>part of speech</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12509</th>\n",
       "      <td>10.1007/s00357-007-0007-9</td>\n",
       "      <td>The Haar Wavelet Transform Dendrogram</td>\n",
       "      <td>We describe new wavelet transform use hierarch...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Information Retrieval</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['wavelet decomposition', 'wavel...</td>\n",
       "      <td>[dendrogram, computability, wavelet decomposit...</td>\n",
       "      <td>[cluster analysis, genetic diversity, computab...</td>\n",
       "      <td>wavelet</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12527</th>\n",
       "      <td>10.1093/comjnl/bxl065</td>\n",
       "      <td>Hedging prediction machine learning</td>\n",
       "      <td>Recent advance machine learning make possible ...</td>\n",
       "      <td>2007</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>87.0</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'syntactic': ['learning objects', 'k-nearest ...</td>\n",
       "      <td>[learning objects, correlation analysis, suppo...</td>\n",
       "      <td>[learning environments, mathematics, clusterin...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12528</th>\n",
       "      <td>10.1142/S1793005708001100</td>\n",
       "      <td>A Neutrosophic Description Logic</td>\n",
       "      <td>Description Logics ( DLs ) appropriate widely ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'syntactic': ['reasoning', 'semantics', 'desc...</td>\n",
       "      <td>[dynamic logic, description logic, multivalued...</td>\n",
       "      <td>[modal logic, formal logic, knowledge represen...</td>\n",
       "      <td>semantics</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>10.1137/S0097539700370084</td>\n",
       "      <td>Setting Parameters Example</td>\n",
       "      <td>We introduce class `` inverse parametric optim...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'syntactic': ['optimization problems', 'path ...</td>\n",
       "      <td>[optimization problems, path planning, optimiz...</td>\n",
       "      <td>[correlation analysis, robots, motion planning...</td>\n",
       "      <td>optimization</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6186 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DOI  \\\n",
       "3      10.1016/j.comgeo.2008.05.005   \n",
       "4         10.1016/j.tcs.2008.03.029   \n",
       "10            10.1093/comjnl/bxm084   \n",
       "14        10.1007/978-3-540-78568-2   \n",
       "15        10.2478/s13230-010-0014-0   \n",
       "...                             ...   \n",
       "12505       10.1109/TED.2007.893191   \n",
       "12509     10.1007/s00357-007-0007-9   \n",
       "12527         10.1093/comjnl/bxl065   \n",
       "12528     10.1142/S1793005708001100   \n",
       "12535     10.1137/S0097539700370084   \n",
       "\n",
       "                                                   Title  \\\n",
       "3                         Edges Switches Tunnels Bridges   \n",
       "4      Soft constraint abstraction based semiring hom...   \n",
       "10                On Ultrametric Algorithmic Information   \n",
       "14     Efficient Algorithms Node Disjoint Subgraph Ho...   \n",
       "15                                  Toward Psycho-robots   \n",
       "...                                                  ...   \n",
       "12505   Retraction Generalized Extension Computing Words   \n",
       "12509              The Haar Wavelet Transform Dendrogram   \n",
       "12527                Hedging prediction machine learning   \n",
       "12528                   A Neutrosophic Description Logic   \n",
       "12535                         Setting Parameters Example   \n",
       "\n",
       "                                                Abstract  Year  \\\n",
       "3      Edge casing well-known method improve readabil...  2009   \n",
       "4      The semiring-based constraint satisfaction pro...  2008   \n",
       "10     How best quantify information object whether n...  2010   \n",
       "14     Recently great effort dedicated research manag...  2008   \n",
       "15     We try perform geometrization psychology repre...  2010   \n",
       "...                                                  ...   ...   \n",
       "12505  Fuzzy automaton whose input alphabet set numbe...  2007   \n",
       "12509  We describe new wavelet transform use hierarch...  2007   \n",
       "12527  Recent advance machine learning make possible ...  2007   \n",
       "12528  Description Logics ( DLs ) appropriate widely ...  2006   \n",
       "12535  We introduce class `` inverse parametric optim...  2003   \n",
       "\n",
       "                             Category  Citation_crossref  Altmetric_score  \\\n",
       "3      Data Structures and Algorithms                7.0            5.488   \n",
       "4             Artificial Intelligence                6.0              NaN   \n",
       "10            Artificial Intelligence                9.0              NaN   \n",
       "14     Data Structures and Algorithms                0.0              NaN   \n",
       "15            Artificial Intelligence                1.0              NaN   \n",
       "...                               ...                ...              ...   \n",
       "12505         Artificial Intelligence                4.0              NaN   \n",
       "12509           Information Retrieval               48.0              NaN   \n",
       "12527                Machine Learning               87.0            4.000   \n",
       "12528         Artificial Intelligence                0.0              NaN   \n",
       "12535  Data Structures and Algorithms               18.0            5.488   \n",
       "\n",
       "       All_mentions  Twitter  News  ...  Patents  Policy  Mendeley  Wikipedia  \\\n",
       "3               2.0      0.0   0.0  ...      0.0     0.0       0.0        0.0   \n",
       "4               NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "10              NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "14              NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "15              NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "...             ...      ...   ...  ...      ...     ...       ...        ...   \n",
       "12505           NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "12509           NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "12527           2.0      1.0   0.0  ...      0.0     0.0       0.0        1.0   \n",
       "12528           NaN      NaN   NaN  ...      NaN     NaN       NaN        NaN   \n",
       "12535           2.0      0.0   0.0  ...      0.0     0.0       0.0        0.0   \n",
       "\n",
       "       Videos                                     CSO_classifier  \\\n",
       "3         0.0  {'syntactic': ['optimization problems', 'optim...   \n",
       "4         NaN  {'syntactic': ['optimal solutions'], 'semantic...   \n",
       "10        NaN  {'syntactic': ['computability'], 'semantic': [...   \n",
       "14        NaN  {'syntactic': ['graph-based', 'synthetic data'...   \n",
       "15        NaN  {'syntactic': ['cognitive systems', 'robots', ...   \n",
       "...       ...                                                ...   \n",
       "12505     NaN  {'syntactic': ['automation', 'feature models',...   \n",
       "12509     NaN  {'syntactic': ['wavelet decomposition', 'wavel...   \n",
       "12527     0.0  {'syntactic': ['learning objects', 'k-nearest ...   \n",
       "12528     NaN  {'syntactic': ['reasoning', 'semantics', 'desc...   \n",
       "12535     0.0  {'syntactic': ['optimization problems', 'path ...   \n",
       "\n",
       "                                       Research_concepts  \\\n",
       "3      [optimization problems, optimization, edge cro...   \n",
       "4      [constraint satisfaction problems (csp), combi...   \n",
       "10     [combinatorial optimization, combinatorial pro...   \n",
       "14     [graph-based, synthetic data, state space, mat...   \n",
       "15     [mobile robots, cognitive systems, cognitive s...   \n",
       "...                                                  ...   \n",
       "12505  [automation, feature models, nondeterministic ...   \n",
       "12509  [dendrogram, computability, wavelet decomposit...   \n",
       "12527  [learning objects, correlation analysis, suppo...   \n",
       "12528  [dynamic logic, description logic, multivalued...   \n",
       "12535  [optimization problems, path planning, optimiz...   \n",
       "\n",
       "                                            Super_topics  \\\n",
       "3      [correlation analysis, mathematics, graph draw...   \n",
       "4      [artificial intelligence, combinatorial mathem...   \n",
       "10     [optimization, combinatorial mathematics, comp...   \n",
       "14     [graphic methods, machine learning, state spac...   \n",
       "15                [robotics, computer science, topology]   \n",
       "...                                                  ...   \n",
       "12505  [engineering, software product line, translati...   \n",
       "12509  [cluster analysis, genetic diversity, computab...   \n",
       "12527  [learning environments, mathematics, clusterin...   \n",
       "12528  [modal logic, formal logic, knowledge represen...   \n",
       "12535  [correlation analysis, robots, motion planning...   \n",
       "\n",
       "                          Name Topic  \n",
       "3                 optimization     0  \n",
       "4       combinatorial problems     1  \n",
       "10      combinatorial problems     1  \n",
       "14          matching algorithm     2  \n",
       "15     artificial intelligence     3  \n",
       "...                        ...   ...  \n",
       "12505           part of speech    67  \n",
       "12509                  wavelet   289  \n",
       "12527         machine learning    38  \n",
       "12528                semantics    16  \n",
       "12535             optimization     0  \n",
       "\n",
       "[6186 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Assign the most frequent concept as the cluster for each paper\n",
    "df[\"Name\"] = df[\"Research_concepts\"].apply(\n",
    "    lambda concepts: max(concepts, key=lambda concept: concept_counts.get(concept, 0))\n",
    ")\n",
    "# Step 3: Generate unique Topic IDs starting from 0\n",
    "df[\"Topic\"] = df[\"Name\"].map({name: idx for idx, name in enumerate(df[\"Name\"].unique())})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcf1dde-1483-40a2-afbc-da214b76b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure feedback columns exist in df, even if filled with 0.0\n",
    "for col in [\"Peers\", \"Expert\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76483d5-9fef-4fea-952f-cf638360a9d9",
   "metadata": {},
   "source": [
    "### 1.2.2 Edges of Impact\n",
    "\n",
    "Edges in the Impact-Augmented Knowledge Graph represent impact-bearing relationships between research topics. An edge is established when two topics co-occur in at least one paper and share a common CSO concept. These relationships are encoded as RDF triples of the form *(TopicPair, hasStageImpact, BlankNode)*.\n",
    "\n",
    "Each `hasStageImpact` predicate is instantiated with a stage-specific property (e.g., `flow:hasReachImpact`, `flow:hasInfluenceImpact`). The associated `BlankNode` stores metadata about the impact source, including:  \n",
    "- `flow:platform`: the platform generating the signal (e.g., Twitter, Facebook)  \n",
    "- `flow:score`: the aggregated and normalised impact score for that platform and stage\n",
    "\n",
    "#### Aggregation Strategy\n",
    "\n",
    "To produce these edge-level scores, we applied a two-step aggregation process:  \n",
    "1. **Normalisation:** Raw scores were independently normalised per platform and dimension to account for scale differences (e.g., Altmetric vs. CrossRef).  \n",
    "2. **Summation:** For each topic pair and stage, we summed the normalised scores across all platforms associated with that dimension.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider the topic pair `Semantics` and `Language Model`, which co-occur and share the CSO concept `language model`. Their Reach impact is encoded as the following RDF triples:  \n",
    "*(flow:pair_15_131, flow:hasReachImpact, :reach1)*,  \n",
    "*(:reach1, flow:platform, \"Twitter\")*,  \n",
    "*(:reach1, flow:score, 0.1321)*,  \n",
    "*(:reach2, flow:platform, \"Facebook\")*,  \n",
    "*(_:reach2, flow:score, 0.0972)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db944d5b-ae67-4c5f-8a32-edb922d4659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(series, range_min, range_max):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    return ((series - min_val) / (max_val - min_val) * (range_max - range_min) + range_min).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0217fea0-00c0-48e6-b35b-de278bc7267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_concepts(concepts):\n",
    "    concepts_count = pd.Series(concepts).value_counts()\n",
    "    top_10_concepts = concepts_count.nlargest(10).index.tolist()\n",
    "    top_10_counts = concepts_count.nlargest(10).values.tolist()\n",
    "    top_10_concepts_with_counts = list(zip(top_10_concepts, top_10_counts))\n",
    "    return top_10_concepts_with_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dedd6d1-5507-4330-9717-72ab046cb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowmetrics labeling\n",
    "reach_columns = [\"Twitter\", \"Facebook\", \"Wikipedia\"]\n",
    "engagement_columns = [\"Blogs\", \"Reddit\", \"Videos\", \"Mendeley\"]\n",
    "feedback_columns = [\"Peers\", \"Expert\"]\n",
    "influence_columns = [\"News\", \"Citation_crossref\"]\n",
    "outcomes_columns = [\"Policy\", \"Patents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e998f90-89f4-41ac-81c7-786098c36fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_scores = df.groupby([\"Topic\", \"Name\"]).agg({\n",
    "    \"Twitter\": \"sum\",\n",
    "    \"Facebook\": \"sum\",\n",
    "    \"Reddit\": \"sum\",\n",
    "    \"Wikipedia\": \"sum\",\n",
    "    \"Blogs\": \"sum\",\n",
    "    \"Videos\": \"sum\",\n",
    "    \"Mendeley\": \"sum\",\n",
    "    \"Peers\": \"sum\",\n",
    "    \"Expert\": \"sum\",\n",
    "    \"News\": \"sum\",\n",
    "    \"Citation_crossref\": \"sum\",\n",
    "    \"Policy\": \"sum\",\n",
    "    \"Patents\": \"sum\",\n",
    "    \"Research_concepts\": lambda x: [concept for sublist in x for concept in sublist]\n",
    "}).reset_index()\n",
    "\n",
    "agg_scores['Count'] = df.groupby([\"Topic\", \"Name\"]).size().values\n",
    "agg_scores['Count_norm'] = normalize(df.groupby([\"Topic\", \"Name\"]).size().values, 40, 100)\n",
    "agg_scores[\"Reach\"] = agg_scores[reach_columns].apply(lambda row: list(zip(reach_columns, row)), axis=1)\n",
    "agg_scores[\"Engagement\"] = agg_scores[engagement_columns].apply(lambda row: list(zip(engagement_columns, row)), axis=1)\n",
    "agg_scores[\"Feedback\"] = agg_scores[feedback_columns].apply(lambda row: list(zip(feedback_columns, row)), axis=1)\n",
    "agg_scores[\"Influence\"] = agg_scores[influence_columns].apply(lambda row: list(zip(influence_columns, row)), axis=1)\n",
    "agg_scores[\"Outcome\"] = agg_scores[outcomes_columns].apply(lambda row: list(zip(outcomes_columns, row)), axis=1)\n",
    "agg_scores[\"Representation\"] = agg_scores[\"Research_concepts\"].apply(get_top_10_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "944c1f4e-b78d-4dfd-8f6a-e65a738919f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Reddit</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Blogs</th>\n",
       "      <th>Videos</th>\n",
       "      <th>Mendeley</th>\n",
       "      <th>Peers</th>\n",
       "      <th>...</th>\n",
       "      <th>Patents</th>\n",
       "      <th>Research_concepts</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count_norm</th>\n",
       "      <th>Reach</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Influence</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>optimization</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>[optimization problems, optimization, edge cro...</td>\n",
       "      <td>356</td>\n",
       "      <td>57</td>\n",
       "      <td>[(Twitter, 1268.0), (Facebook, 21.0), (Wikiped...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 6.0), (Videos, 1.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 10812.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 55.0)]</td>\n",
       "      <td>[(optimization, 356), (optimization problems, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>combinatorial problems</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[constraint satisfaction problems (csp), combi...</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>[(Twitter, 103.0), (Facebook, 2.0), (Wikipedia...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 3.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 1294.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 8.0)]</td>\n",
       "      <td>[(combinatorial problems, 38), (combinatorial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>matching algorithm</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[graph-based, synthetic data, state space, mat...</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 31.0), (Facebook, 2.0), (Wikipedia,...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 169.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(matching algorithm, 6), (matching methods, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>2849.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>[mobile robots, cognitive systems, cognitive s...</td>\n",
       "      <td>157</td>\n",
       "      <td>47</td>\n",
       "      <td>[(Twitter, 2849.0), (Facebook, 11.0), (Wikiped...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 15.0), (Videos, 2.0), ...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 4861.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 35.0)]</td>\n",
       "      <td>[(artificial intelligence, 157), (expert syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>probability</td>\n",
       "      <td>266.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[memetic, estimation of distribution algorithm...</td>\n",
       "      <td>59</td>\n",
       "      <td>42</td>\n",
       "      <td>[(Twitter, 266.0), (Facebook, 9.0), (Wikipedia...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 6.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 1338.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 15.0)]</td>\n",
       "      <td>[(probability, 59), (probability distributions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>244</td>\n",
       "      <td>medical images</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[medical images, clutter (information theory),...</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 7.0), (Facebook, 0.0), (Wikipedia, ...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 53.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(medical images, 2), (clutter (information th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>247</td>\n",
       "      <td>virtual reality</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[virtual reality, virtual worlds, knowledge tr...</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 22.0), (Facebook, 0.0), (Wikipedia,...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 92.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(virtual reality, 6), (virtual environments, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>264</td>\n",
       "      <td>autonomous driving</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[scene understanding, different frequency, fre...</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 7.0), (Facebook, 0.0), (Wikipedia, ...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 23.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(autonomous driving, 2), (scene understanding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>272</td>\n",
       "      <td>embedded systems</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[grammatical evolution, l2 cache, memory syste...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 3.0), (Facebook, 0.0), (Wikipedia, ...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 4.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(grammatical evolution, 1), (l2 cache, 1), (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>289</td>\n",
       "      <td>wavelet</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[nonrigid registration, wavelet, fourier trans...</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>[(Twitter, 1.0), (Facebook, 0.0), (Wikipedia, ...</td>\n",
       "      <td>[(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...</td>\n",
       "      <td>[(Peers, 0.0), (Expert, 0.0)]</td>\n",
       "      <td>[(News, 0.0), (Citation_crossref, 71.0)]</td>\n",
       "      <td>[(Policy, 0.0), (Patents, 0.0)]</td>\n",
       "      <td>[(wavelet, 2), (wavelet transforms, 2), (nonri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic                     Name  Twitter  Facebook  Reddit  Wikipedia  \\\n",
       "0        0             optimization   1268.0      21.0     6.0       39.0   \n",
       "1        1   combinatorial problems    103.0       2.0     3.0        5.0   \n",
       "2        2       matching algorithm     31.0       2.0     0.0        0.0   \n",
       "3        3  artificial intelligence   2849.0      11.0    15.0       24.0   \n",
       "4        4              probability    266.0       9.0     6.0        3.0   \n",
       "..     ...                      ...      ...       ...     ...        ...   \n",
       "152    244           medical images      7.0       0.0     0.0        0.0   \n",
       "153    247          virtual reality     22.0       0.0     0.0        0.0   \n",
       "154    264       autonomous driving      7.0       0.0     0.0        0.0   \n",
       "155    272         embedded systems      3.0       0.0     0.0        0.0   \n",
       "156    289                  wavelet      1.0       0.0     0.0        0.0   \n",
       "\n",
       "     Blogs  Videos  Mendeley  Peers  ...  Patents  \\\n",
       "0      0.0     1.0       0.0    0.0  ...     55.0   \n",
       "1      0.0     0.0       0.0    0.0  ...      8.0   \n",
       "2      0.0     0.0       0.0    0.0  ...      0.0   \n",
       "3      0.0     2.0       0.0    0.0  ...     35.0   \n",
       "4      0.0     0.0       0.0    0.0  ...     15.0   \n",
       "..     ...     ...       ...    ...  ...      ...   \n",
       "152    0.0     0.0       0.0    0.0  ...      0.0   \n",
       "153    0.0     0.0       0.0    0.0  ...      0.0   \n",
       "154    0.0     0.0       0.0    0.0  ...      0.0   \n",
       "155    0.0     0.0       0.0    0.0  ...      0.0   \n",
       "156    0.0     0.0       0.0    0.0  ...      0.0   \n",
       "\n",
       "                                     Research_concepts  Count  Count_norm  \\\n",
       "0    [optimization problems, optimization, edge cro...    356          57   \n",
       "1    [constraint satisfaction problems (csp), combi...     38          41   \n",
       "2    [graph-based, synthetic data, state space, mat...      6          40   \n",
       "3    [mobile robots, cognitive systems, cognitive s...    157          47   \n",
       "4    [memetic, estimation of distribution algorithm...     59          42   \n",
       "..                                                 ...    ...         ...   \n",
       "152  [medical images, clutter (information theory),...      2          40   \n",
       "153  [virtual reality, virtual worlds, knowledge tr...      6          40   \n",
       "154  [scene understanding, different frequency, fre...      2          40   \n",
       "155  [grammatical evolution, l2 cache, memory syste...      1          40   \n",
       "156  [nonrigid registration, wavelet, fourier trans...      2          40   \n",
       "\n",
       "                                                 Reach  \\\n",
       "0    [(Twitter, 1268.0), (Facebook, 21.0), (Wikiped...   \n",
       "1    [(Twitter, 103.0), (Facebook, 2.0), (Wikipedia...   \n",
       "2    [(Twitter, 31.0), (Facebook, 2.0), (Wikipedia,...   \n",
       "3    [(Twitter, 2849.0), (Facebook, 11.0), (Wikiped...   \n",
       "4    [(Twitter, 266.0), (Facebook, 9.0), (Wikipedia...   \n",
       "..                                                 ...   \n",
       "152  [(Twitter, 7.0), (Facebook, 0.0), (Wikipedia, ...   \n",
       "153  [(Twitter, 22.0), (Facebook, 0.0), (Wikipedia,...   \n",
       "154  [(Twitter, 7.0), (Facebook, 0.0), (Wikipedia, ...   \n",
       "155  [(Twitter, 3.0), (Facebook, 0.0), (Wikipedia, ...   \n",
       "156  [(Twitter, 1.0), (Facebook, 0.0), (Wikipedia, ...   \n",
       "\n",
       "                                            Engagement  \\\n",
       "0    [(Blogs, 0.0), (Reddit, 6.0), (Videos, 1.0), (...   \n",
       "1    [(Blogs, 0.0), (Reddit, 3.0), (Videos, 0.0), (...   \n",
       "2    [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "3    [(Blogs, 0.0), (Reddit, 15.0), (Videos, 2.0), ...   \n",
       "4    [(Blogs, 0.0), (Reddit, 6.0), (Videos, 0.0), (...   \n",
       "..                                                 ...   \n",
       "152  [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "153  [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "154  [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "155  [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "156  [(Blogs, 0.0), (Reddit, 0.0), (Videos, 0.0), (...   \n",
       "\n",
       "                          Feedback  \\\n",
       "0    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "1    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "2    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "3    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "4    [(Peers, 0.0), (Expert, 0.0)]   \n",
       "..                             ...   \n",
       "152  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "153  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "154  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "155  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "156  [(Peers, 0.0), (Expert, 0.0)]   \n",
       "\n",
       "                                       Influence  \\\n",
       "0    [(News, 0.0), (Citation_crossref, 10812.0)]   \n",
       "1     [(News, 0.0), (Citation_crossref, 1294.0)]   \n",
       "2      [(News, 0.0), (Citation_crossref, 169.0)]   \n",
       "3     [(News, 0.0), (Citation_crossref, 4861.0)]   \n",
       "4     [(News, 0.0), (Citation_crossref, 1338.0)]   \n",
       "..                                           ...   \n",
       "152     [(News, 0.0), (Citation_crossref, 53.0)]   \n",
       "153     [(News, 0.0), (Citation_crossref, 92.0)]   \n",
       "154     [(News, 0.0), (Citation_crossref, 23.0)]   \n",
       "155      [(News, 0.0), (Citation_crossref, 4.0)]   \n",
       "156     [(News, 0.0), (Citation_crossref, 71.0)]   \n",
       "\n",
       "                              Outcome  \\\n",
       "0    [(Policy, 0.0), (Patents, 55.0)]   \n",
       "1     [(Policy, 0.0), (Patents, 8.0)]   \n",
       "2     [(Policy, 0.0), (Patents, 0.0)]   \n",
       "3    [(Policy, 0.0), (Patents, 35.0)]   \n",
       "4    [(Policy, 0.0), (Patents, 15.0)]   \n",
       "..                                ...   \n",
       "152   [(Policy, 0.0), (Patents, 0.0)]   \n",
       "153   [(Policy, 0.0), (Patents, 0.0)]   \n",
       "154   [(Policy, 0.0), (Patents, 0.0)]   \n",
       "155   [(Policy, 0.0), (Patents, 0.0)]   \n",
       "156   [(Policy, 0.0), (Patents, 0.0)]   \n",
       "\n",
       "                                        Representation  \n",
       "0    [(optimization, 356), (optimization problems, ...  \n",
       "1    [(combinatorial problems, 38), (combinatorial ...  \n",
       "2    [(matching algorithm, 6), (matching methods, 5...  \n",
       "3    [(artificial intelligence, 157), (expert syste...  \n",
       "4    [(probability, 59), (probability distributions...  \n",
       "..                                                 ...  \n",
       "152  [(medical images, 2), (clutter (information th...  \n",
       "153  [(virtual reality, 6), (virtual environments, ...  \n",
       "154  [(autonomous driving, 2), (scene understanding...  \n",
       "155  [(grammatical evolution, 1), (l2 cache, 1), (m...  \n",
       "156  [(wavelet, 2), (wavelet transforms, 2), (nonri...  \n",
       "\n",
       "[157 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove topics where the 'Representation' list has fewer than 10 concepts\n",
    "agg_scores = agg_scores[agg_scores[\"Representation\"].apply(lambda x: len(x) == 10)].reset_index(drop=True)\n",
    "agg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff1a22f4-31f9-498c-ba6e-b0db01a1cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "\n",
    "for idx, row in agg_scores.iterrows():\n",
    "    topics[idx] = {\n",
    "        'topic': row['Name'],\n",
    "        'cluster_size': row['Count_norm'],\n",
    "        'reach': row['Reach'],\n",
    "        'engagement': row['Engagement'],\n",
    "        'feedback': row['Feedback'],\n",
    "        'influence': row['Influence'],\n",
    "        'outcome': row['Outcome'],\n",
    "        'words': row['Representation']  # List of tuples (concept, count)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f7fe5-48b4-426a-b64c-d0e2419a1d4a",
   "metadata": {},
   "source": [
    "### 1.2.3 Impact-augmented knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5ec8c71-d107-48e0-b5a7-4cf2cd043bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FILE = project_dir / \"data\" / \"impact_augmented_kg.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "134a5fc1-74a5-4260-88be-c13aa811ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_edge_weights(G, weight_attr=\"width\", new_range=(0, 1)):\n",
    "    a, b = new_range\n",
    "    weights = [G[u][v][weight_attr] for u, v in G.edges]\n",
    "    min_weight = min(weights)\n",
    "    max_weight = max(weights)\n",
    "    for u, v in G.edges:\n",
    "        original_weight = G[u][v][weight_attr]\n",
    "        normalized_weight = a + ((original_weight - min_weight) * (b - a)) / (max_weight - min_weight)\n",
    "        G[u][v][weight_attr] = round(normalized_weight, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22cbcf24-5216-490d-9af0-d87e12cc69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for topic, value in topics.items():\n",
    "    topic_node = f\"Topic {topic}\"\n",
    "    G.add_node(topic_node, size=value['cluster_size'], label=value['topic'], type='topic')\n",
    "    \n",
    "    for word, _ in value.get(\"words\", [])[:10]: #[:100]\n",
    "        G.add_node(word, size=10, label=word, type='leaf')\n",
    "        G.add_edge(topic_node, word, weight=1)\n",
    "\n",
    "overlap_leafs = []\n",
    "for topic1, value1 in topics.items():\n",
    "    for topic2, value2 in topics.items():\n",
    "        if topic1 != topic2:\n",
    "            words1 = set([word[0] for word in value1.get(\"words\", [])])\n",
    "            words2 = set([word[0] for word in value2.get(\"words\", [])])\n",
    "            common_words = words1.intersection(words2)\n",
    "            if common_words:\n",
    "                overlap_leafs.append(next(iter(common_words)))\n",
    "                G.add_edge(f\"Topic {topic1}\", f\"Topic {topic2}\", color=\"#4caf50\", weight=len(common_words) + \n",
    "                           float(sum(value for _, value in value1.get(\"reach\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"reach\", []))) + \n",
    "                           float(sum(value for _, value in value1.get(\"engagement\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"engagement\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"feedback\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"feedback\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"influence\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"influence\", []))) +\n",
    "                           float(sum(value for _, value in value1.get(\"outcome\", []))) + \n",
    "                           float(sum(value for _, value in value2.get(\"outcome\", []))))\n",
    "\n",
    "# Normalize weights\n",
    "normalize_edge_weights(G, weight_attr=\"weight\", new_range=(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca74bdb2-71ce-470b-9893-e1f7f6ddd440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "G_topic_knowledge_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"G_topic_knowledge_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x352cfdd00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(height=\"800px\", width=\"100%\", notebook=True, bgcolor=\"white\", font_color=\"black\") ##f0f0f0\n",
    "net.from_nx(G)\n",
    "\n",
    "for node in net.nodes:\n",
    "    if node['type'] == \"topic\":\n",
    "        node['borderWidth'] = 4\n",
    "        node['shadow'] = True\n",
    "        node['color'] = \"#9e9e9e\"\n",
    "    else:\n",
    "        if node['id'] in overlap_leafs:\n",
    "            node['borderWidth'] = 3\n",
    "            node['shadow'] = True\n",
    "            node['color'] = \"#ff9800\"\n",
    "        else:\n",
    "            node['borderWidth'] = 3\n",
    "            node['shadow'] = True\n",
    "            node['color'] = \"#616161\"\n",
    "\n",
    "for edge in net.edges:\n",
    "    weight = edge.get('width', 1)\n",
    "    edge['label'] = str(weight)\n",
    "    edge['font'] = {'size': 8}\n",
    "    \n",
    "net.force_atlas_2based()\n",
    "net.show(\"G_topic_knowledge_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "466218f3-023c-4e59-9728-25c214506293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF TTL file saved.\n"
     ]
    }
   ],
   "source": [
    "# Set up RDF graph and namespaces\n",
    "rdf_graph = RDFGraph()\n",
    "FLOW = Namespace(\"http://example.org/flowmetrics#\")\n",
    "SKOS = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "CSO = Namespace(\"http://cso.kmi.open.ac.uk/schema/cso#\")\n",
    "\n",
    "rdf_graph.bind(\"flow\", FLOW)\n",
    "rdf_graph.bind(\"skos\", SKOS)\n",
    "rdf_graph.bind(\"cso\", CSO)\n",
    "rdf_graph.bind(\"rdfs\", RDFS)\n",
    "rdf_graph.bind(\"xsd\", XSD)\n",
    "\n",
    "def extract_topic_id(label):\n",
    "    return label.replace(\"Topic \", \"\").strip()  # returns string like \"42\"\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data[\"type\"] == \"topic\":\n",
    "        topic_id = extract_topic_id(node)\n",
    "        topic_uri = URIRef(FLOW + f\"topic_{topic_id}\")\n",
    "        rdf_graph.add((topic_uri, RDF.type, FLOW.ResearchTopic))\n",
    "        rdf_graph.add((topic_uri, RDF.type, SKOS.Concept))\n",
    "        rdf_graph.add((topic_uri, RDFS.label, Literal(data[\"label\"], datatype=XSD.string)))\n",
    "\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if G.nodes[u][\"type\"] == \"topic\" and G.nodes[v][\"type\"] == \"topic\":\n",
    "        id1, id2 = extract_topic_id(u), extract_topic_id(v)\n",
    "        topic_uri1 = URIRef(FLOW + f\"topic_{id1}\")\n",
    "        topic_uri2 = URIRef(FLOW + f\"topic_{id2}\")\n",
    "        sorted_ids = sorted([int(id1), int(id2)])\n",
    "        pair_uri = URIRef(FLOW + f\"pair_{sorted_ids[0]}_{sorted_ids[1]}\")\n",
    "\n",
    "        rdf_graph.add((pair_uri, RDF.type, FLOW.TopicPair))\n",
    "        rdf_graph.add((pair_uri, FLOW.hasTopic, topic_uri1))\n",
    "        rdf_graph.add((pair_uri, FLOW.hasTopic, topic_uri2))\n",
    "\n",
    "        words1 = set(w[0] for w in topics[int(id1)].get(\"words\", []))\n",
    "        words2 = set(w[0] for w in topics[int(id2)].get(\"words\", []))\n",
    "        shared = sorted(words1 & words2)\n",
    "        for concept in shared[:3]:\n",
    "            rdf_graph.add((pair_uri, FLOW.hasSharedConcept, Literal(concept, datatype=XSD.string)))\n",
    "\n",
    "        for stage in impact_stages:\n",
    "            platform_scores = {}\n",
    "            for topic_id in [int(id1), int(id2)]:\n",
    "                for platform, score in topics[topic_id].get(stage, []):\n",
    "                    platform_scores[platform] = platform_scores.get(platform, 0.0) + score\n",
    "\n",
    "            for platform, score in platform_scores.items():\n",
    "                bnode = BNode()\n",
    "                rdf_graph.add((pair_uri, FLOW[f\"has{stage.capitalize()}Impact\"], bnode))\n",
    "                rdf_graph.add((bnode, FLOW.score, Literal(score, datatype=XSD.float)))\n",
    "                rdf_graph.add((bnode, FLOW.platform, Literal(platform, datatype=XSD.string)))\n",
    "\n",
    "# Save TTL\n",
    "rdf_graph.serialize(destination=str(GRAPH_FILE), format=\"turtle\")\n",
    "print(f\"RDF TTL file saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
